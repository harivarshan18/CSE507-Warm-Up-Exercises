{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c47ee466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean imports & config ---\n",
    "from pathlib import Path\n",
    "import zipfile, urllib.request, random, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BASE_DIR = Path(\"./data/jsrt\").resolve()\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_if_needed(name: str, url: str, dst_dir: Path = BASE_DIR) -> Path:\n",
    "    \"\"\"Downloads <name>.zip to dst_dir and extracts into dst_dir/<name>.\"\"\"\n",
    "    out_dir = dst_dir / name\n",
    "    zip_path = dst_dir / f\"{name}.zip\"\n",
    "    if out_dir.exists() and any(out_dir.iterdir()):\n",
    "        print(f\"[{name}] ready at {out_dir}\")\n",
    "        return out_dir\n",
    "    if not zip_path.exists():\n",
    "        print(f\"[DL] {name} -> {zip_path}\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "    print(f\"[EXTRACT] {zip_path} -> {out_dir}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "# ImageNet-normalized transforms (classification)\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "\n",
    "def get_imagenet_preprocess(weights):\n",
    "    try:\n",
    "        return weights.transforms()  # callable; works in newer torchvision\n",
    "    except Exception:\n",
    "        # Fallback for older torchvision\n",
    "        from torchvision.transforms import InterpolationMode\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(232, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "        ])\n",
    "\n",
    "preprocess = get_imagenet_preprocess(weights)\n",
    "\n",
    "cls_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    preprocess,  # keep as a single callable; don't try to access .transforms\n",
    "])\n",
    "\n",
    "# --- Generic training helpers (classification) ---\n",
    "def build_resnet18(num_classes: int, freeze_backbone: bool = True) -> nn.Module:\n",
    "    m = resnet18(weights=weights)\n",
    "    if freeze_backbone:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = False\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m.to(DEVICE)\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    train_mode = optimizer is not None\n",
    "    model.train(train_mode)\n",
    "    losses, preds_all, targs_all = 0.0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        if train_mode: optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        if train_mode:\n",
    "            loss.backward(); optimizer.step()\n",
    "        losses += loss.item() * x.size(0)\n",
    "        preds_all.extend(out.argmax(1).detach().cpu().tolist())\n",
    "        targs_all.extend(y.detach().cpu().tolist())\n",
    "    return losses / len(loader.dataset), accuracy_score(targs_all, preds_all)\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    best_acc, best = -1.0, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, crit, optimizer=opt)\n",
    "        va_loss, va_acc = run_epoch(model, val_loader,   crit, optimizer=None)\n",
    "        if va_acc > best_acc:\n",
    "            best_acc, best = va_acc, {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        print(f\"E{ep:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f} | {time.time()-t0:.1f}s\")\n",
    "    if best is not None:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best.items()})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd74f6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Directions01] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Directions01\n",
      "Classes (Task 1): ['down', 'left', 'right', 'up']\n",
      "E01 | train 0.8438/0.744 | val 0.7814/0.750 | 21.8s\n",
      "E02 | train 0.3535/0.948 | val 0.3101/0.975 | 13.1s\n",
      "E03 | train 0.2171/0.985 | val 0.1693/0.975 | 10.5s\n",
      "E04 | train 0.1685/0.981 | val 0.1229/1.000 | 12.6s\n",
      "E05 | train 0.1270/0.994 | val 0.1141/0.975 | 10.5s\n",
      "[Task 1] Test loss: 0.1229 | Test acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Directions01\n",
    "DATASETS = {\n",
    "    \"Directions01\": \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Directions01.zip\",\n",
    "}\n",
    "\n",
    "directions_dir = download_if_needed(\"Directions01\", DATASETS[\"Directions01\"])\n",
    "train_dir_1 = directions_dir / \"train\"   # expects subfolders: Up, Down, Left, Right\n",
    "test_dir_1  = directions_dir / \"test\"\n",
    "\n",
    "train_ds_1 = datasets.ImageFolder(train_dir_1, transform=cls_tfms)\n",
    "test_ds_1  = datasets.ImageFolder(test_dir_1,  transform=cls_tfms)\n",
    "print(\"Classes (Task 1):\", train_ds_1.classes)\n",
    "\n",
    "num_workers = 2\n",
    "train_ld_1 = DataLoader(train_ds_1, batch_size=32, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
    "test_ld_1  = DataLoader(test_ds_1,  batch_size=64, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model_1 = build_resnet18(num_classes=4, freeze_backbone=True)\n",
    "model_1 = fit(model_1, train_ld_1, test_ld_1, epochs=5, lr=1e-3)\n",
    "\n",
    "test_loss_1, test_acc_1 = run_epoch(model_1, test_ld_1, nn.CrossEntropyLoss(), optimizer=None)\n",
    "print(f\"[Task 1] Test loss: {test_loss_1:.4f} | Test acc: {test_acc_1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba3515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gender01] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Gender01\n",
      "Classes (Task 2): ['female', 'male']\n",
      "E01 | train 0.6466/0.610 | val 0.6249/0.667 | 7.2s\n",
      "E02 | train 0.5214/0.831 | val 0.7319/0.473 | 1.0s\n",
      "E03 | train 0.4514/0.805 | val 0.5267/0.806 | 0.9s\n",
      "E04 | train 0.3908/0.896 | val 0.5954/0.634 | 1.1s\n",
      "E05 | train 0.3824/0.870 | val 0.4635/0.882 | 0.9s\n",
      "E06 | train 0.2969/0.955 | val 0.4784/0.806 | 0.8s\n",
      "[Task 2] Test loss: 0.4635 | Test acc: 0.882\n"
     ]
    }
   ],
   "source": [
    "# Gender01\n",
    "DATASETS[\"Gender01\"] = \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Gender01.zip\"\n",
    "\n",
    "gender_dir = download_if_needed(\"Gender01\", DATASETS[\"Gender01\"])\n",
    "train_dir_2 = gender_dir / \"train\"   # subfolders: female, male\n",
    "test_dir_2  = gender_dir / \"test\"\n",
    "\n",
    "train_ds_2 = datasets.ImageFolder(train_dir_2, transform=cls_tfms)\n",
    "test_ds_2  = datasets.ImageFolder(test_dir_2,  transform=cls_tfms)\n",
    "print(\"Classes (Task 2):\", train_ds_2.classes)\n",
    "\n",
    "train_ld_2 = DataLoader(train_ds_2, batch_size=16, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
    "test_ld_2  = DataLoader(test_ds_2,  batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model_2 = build_resnet18(num_classes=2, freeze_backbone=True)\n",
    "model_2 = fit(model_2, train_ld_2, test_ld_2, epochs=6, lr=1e-3)\n",
    "\n",
    "test_loss_2, test_acc_2 = run_epoch(model_2, test_ld_2, nn.CrossEntropyLoss(), optimizer=None)\n",
    "print(f\"[Task 2] Test loss: {test_loss_2:.4f} | Test acc: {test_acc_2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ea7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'age_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pref[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m pref \u001b[38;5;28;01melse\u001b[39;00m cands[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 1) Find + read CSV\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m csv_file \u001b[38;5;241m=\u001b[39m find_first_csv(\u001b[43mage_dir\u001b[49m)\n\u001b[1;32m     13\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Age] CSV columns:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'age_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Task 3: Age estimation (XPAge01_RGB) ---\n",
    "# Dataset spec: 247 JPEGs (2048×2048) + CSV with ages.  (miniJSRT XPAge01_RGB)\n",
    "\n",
    "age_dir = download_if_needed(\n",
    "    \"XPAge01_RGB\",\n",
    "    \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/XPAge01_RGB.zip\"\n",
    ")\n",
    "csv_path = find_first_csv(age_dir)\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Try to auto-detect column names\n",
    "fname_col = next((c for c in df.columns if c.lower().startswith(\"file\")), df.columns[0])\n",
    "age_col   = next((c for c in df.columns if \"age\" in c.lower()), df.columns[-1])\n",
    "\n",
    "def resolve_img(root: Path, fname: str) -> Path:\n",
    "    p = root / fname\n",
    "    if p.exists(): return p\n",
    "    hits = list(root.rglob(fname))\n",
    "    if hits: return hits[0]\n",
    "    raise FileNotFoundError(f\"Image {fname} not found under {root}\")\n",
    "\n",
    "IMAGENET = ResNet18_Weights.DEFAULT\n",
    "age_tfms  = build_img_tfms(size=224, grayscale=True)\n",
    "\n",
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, table, img_root: Path, fname_col: str, age_col: str, transform=None):\n",
    "        self.table = table.reset_index(drop=True)\n",
    "        self.img_root = img_root\n",
    "        self.fname_col = fname_col\n",
    "        self.age_col = age_col\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.table)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.table.iloc[i]\n",
    "        img = Image.open(resolve_img(self.img_root, str(row[self.fname_col]))).convert(\"RGB\")\n",
    "        if self.transform: img = self.transform(img)\n",
    "        age = torch.tensor([float(row[self.age_col])], dtype=torch.float32)\n",
    "        return img, age\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=SEED)\n",
    "ds_tr = AgeDataset(train_df, age_dir, fname_col, age_col, transform=age_tfms)\n",
    "ds_te = AgeDataset(test_df,  age_dir, fname_col, age_col, transform=age_tfms)\n",
    "ld_tr = DataLoader(ds_tr, batch_size=8, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "ld_te = DataLoader(ds_te, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "model_age = resnet18(weights=IMAGENET)\n",
    "for p in model_age.parameters(): p.requires_grad = False   # start by freezing backbone\n",
    "model_age.fc = nn.Linear(model_age.fc.in_features, 1)\n",
    "model_age = model_age.to(DEVICE)\n",
    "\n",
    "fit_regression(model_age, ld_tr, ld_te, epochs=5, lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Segmentation01] Downloading -> /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation01.zip\n",
      "[Segmentation01] Extracting...\n",
      "[Segmentation01] Ready at: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation01\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'age_tfms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m org_train \u001b[38;5;241m=\u001b[39m seg1_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg_train\u001b[39m\u001b[38;5;124m\"\u001b[39m; lbl_train \u001b[38;5;241m=\u001b[39m seg1_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_train\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m org_test  \u001b[38;5;241m=\u001b[39m seg1_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg_test\u001b[39m\u001b[38;5;124m\"\u001b[39m;  lbl_test  \u001b[38;5;241m=\u001b[39m seg1_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 23\u001b[0m train_ds_4 \u001b[38;5;241m=\u001b[39m LungSegDataset(org_train, lbl_train, transform\u001b[38;5;241m=\u001b[39m\u001b[43mage_tfms\u001b[49m)\n\u001b[1;32m     24\u001b[0m test_ds_4  \u001b[38;5;241m=\u001b[39m LungSegDataset(org_test,  lbl_test,  transform\u001b[38;5;241m=\u001b[39mage_tfms)\n\u001b[1;32m     25\u001b[0m train_ld_4 \u001b[38;5;241m=\u001b[39m DataLoader(train_ds_4, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'age_tfms' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Task 4: Segmentation01 (lung vs background) ---\n",
    "# Spec: 256×256 PNGs; train=50, test=10; masks: lung=255, background=0.\n",
    "\n",
    "seg1_dir = download_if_needed(\n",
    "    \"Segmentation01\",\n",
    "    \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Segmentation01.zip\"\n",
    ")\n",
    "org_train = seg1_dir / \"org_train\"\n",
    "lbl_train = seg1_dir / \"label_train\"\n",
    "org_test  = seg1_dir / \"org_test\"\n",
    "lbl_test  = seg1_dir / \"label_test\"\n",
    "\n",
    "img_tfms_seg = build_img_tfms(size=256, grayscale=True)\n",
    "\n",
    "class LungSegDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.imgs  = sorted(list(img_dir.glob(\"*.*\")))\n",
    "        self.masks = sorted(list(mask_dir.glob(\"*.*\")))\n",
    "        assert len(self.imgs)==len(self.masks), \"Image/mask count mismatch\"\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.imgs)\n",
    "    def __getitem__(self, i):\n",
    "        img  = Image.open(self.imgs[i]).convert(\"RGB\")\n",
    "        m    = Image.open(self.masks[i]).convert(\"L\")\n",
    "        img  = self.transform(img) if self.transform else transforms.ToTensor()(img)\n",
    "        mask = torch.from_numpy((np.array(m)>0).astype(np.int64))   # -> {0,1}\n",
    "        return img, mask\n",
    "\n",
    "ds_tr4 = LungSegDataset(org_train, lbl_train, img_tfms_seg)\n",
    "ds_te4 = LungSegDataset(org_test,  lbl_test,  img_tfms_seg)\n",
    "ld_tr4 = DataLoader(ds_tr4, batch_size=4, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "ld_te4 = DataLoader(ds_te4, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "model_seg_bin = fcn_resnet50(weights=FCN_ResNet50_Weights.DEFAULT, num_classes=2).to(DEVICE)\n",
    "fit_segmentation(model_seg_bin, ld_tr4, epochs=3, lr=1e-4)\n",
    "\n",
    "miou4 = mean_iou(model_seg_bin, ld_te4, num_classes=2)\n",
    "print(f\"[Task 4] mean IoU: {miou4:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1030c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Task 5: Segmentation02 (multi-class organs) ---\n",
    "# Spec: 256×256 BMP inputs; label PNG with values {0 (outside body), 170 (chest area), 255 (lungs), 85 (heart)}.\n",
    "# Map to contiguous class IDs {0,1,2,3} for CrossEntropyLoss.\n",
    "\n",
    "seg2_dir = download_if_needed(\n",
    "    \"Segmentation02\",\n",
    "    \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/segmentation02.zip\"\n",
    ")\n",
    "org_train2 = seg2_dir / \"org_train\"\n",
    "lbl_train2 = seg2_dir / \"label_train\"\n",
    "org_test2  = seg2_dir / \"org_test\"\n",
    "lbl_test2  = seg2_dir / \"label_test\"\n",
    "\n",
    "VAL2IDX = {0:0, 170:1, 255:2, 85:3}\n",
    "\n",
    "img_tfms_seg2 = build_img_tfms(size=256, grayscale=True)\n",
    "\n",
    "def remap_mask(arr: np.ndarray, mapping=VAL2IDX) -> np.ndarray:\n",
    "    out = np.zeros_like(arr, dtype=np.int64)\n",
    "    for v,i in mapping.items():\n",
    "        out[arr==v] = i\n",
    "    return out\n",
    "\n",
    "class OrganSegDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, transform=None):\n",
    "        self.imgs  = sorted(list(img_dir.glob(\"*.*\")))   # BMP inputs\n",
    "        self.masks = sorted(list(mask_dir.glob(\"*.*\")))  # PNG labels\n",
    "        assert len(self.imgs)==len(self.masks), \"Image/mask count mismatch\"\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.imgs)\n",
    "    def __getitem__(self, i):\n",
    "        img  = Image.open(self.imgs[i]).convert(\"RGB\")\n",
    "        m    = Image.open(self.masks[i]).convert(\"L\")\n",
    "        img  = self.transform(img) if self.transform else transforms.ToTensor()(img)\n",
    "        mask = torch.from_numpy(remap_mask(np.array(m, dtype=np.int64)))  # -> {0..3}\n",
    "        return img, mask\n",
    "\n",
    "ds_tr5 = OrganSegDataset(org_train2, lbl_train2, img_tfms_seg2)\n",
    "ds_te5 = OrganSegDataset(org_test2,  lbl_test2,  img_tfms_seg2)\n",
    "ld_tr5 = DataLoader(ds_tr5, batch_size=2, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "ld_te5 = DataLoader(ds_te5, batch_size=2, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "model_seg_multi = fcn_resnet50(weights=FCN_ResNet50_Weights.DEFAULT, num_classes=4).to(DEVICE)\n",
    "fit_segmentation(model_seg_multi, ld_tr5, epochs=3, lr=1e-4)\n",
    "\n",
    "miou5 = mean_iou(model_seg_multi, ld_te5, num_classes=4)\n",
    "print(f\"[Task 5] mean IoU: {miou5:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse507",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
