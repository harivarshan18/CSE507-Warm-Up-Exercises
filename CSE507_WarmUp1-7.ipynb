{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c47ee466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Clean imports & config ---\n",
    "from pathlib import Path\n",
    "import zipfile, urllib.request, random, time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchvision.models.segmentation import fcn_resnet50, FCN_ResNet50_Weights\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import random\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "BASE_DIR = Path(\"./data/jsrt\").resolve()\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_if_needed(name: str, url: str, dst_dir: Path = BASE_DIR) -> Path:\n",
    "    \"\"\"Downloads <name>.zip to dst_dir and extracts into dst_dir/<name>.\"\"\"\n",
    "    out_dir = dst_dir / name\n",
    "    zip_path = dst_dir / f\"{name}.zip\"\n",
    "    if out_dir.exists() and any(out_dir.iterdir()):\n",
    "        print(f\"[{name}] ready at {out_dir}\")\n",
    "        return out_dir\n",
    "    if not zip_path.exists():\n",
    "        print(f\"[DL] {name} -> {zip_path}\")\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "    print(f\"[EXTRACT] {zip_path} -> {out_dir}\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "        zf.extractall(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "# ImageNet-normalized transforms (classification)\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "\n",
    "def get_imagenet_preprocess(weights):\n",
    "    try:\n",
    "        return weights.transforms()  # callable; works in newer torchvision\n",
    "    except Exception:\n",
    "        # Fallback for older torchvision\n",
    "        from torchvision.transforms import InterpolationMode\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(232, interpolation=InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "        ])\n",
    "\n",
    "preprocess = get_imagenet_preprocess(weights)\n",
    "\n",
    "cls_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    preprocess,  \n",
    "])\n",
    "\n",
    "# --- Generic training helpers (classification) ---\n",
    "def build_resnet18(num_classes: int, freeze_backbone: bool = True) -> nn.Module:\n",
    "    m = resnet18(weights=weights)\n",
    "    if freeze_backbone:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = False\n",
    "    m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
    "    return m.to(DEVICE)\n",
    "\n",
    "def run_epoch(model, loader, criterion, optimizer=None):\n",
    "    train_mode = optimizer is not None\n",
    "    model.train(train_mode)\n",
    "    losses, preds_all, targs_all = 0.0, [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        if train_mode: optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        if train_mode:\n",
    "            loss.backward(); optimizer.step()\n",
    "        losses += loss.item() * x.size(0)\n",
    "        preds_all.extend(out.argmax(1).detach().cpu().tolist())\n",
    "        targs_all.extend(y.detach().cpu().tolist())\n",
    "    return losses / len(loader.dataset), accuracy_score(targs_all, preds_all)\n",
    "\n",
    "def fit(model, train_loader, val_loader, epochs=5, lr=1e-3):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    best_acc, best = -1.0, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, crit, optimizer=opt)\n",
    "        va_loss, va_acc = run_epoch(model, val_loader,   crit, optimizer=None)\n",
    "        if va_acc > best_acc:\n",
    "            best_acc, best = va_acc, {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        print(f\"E{ep:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f} | {time.time()-t0:.1f}s\")\n",
    "    if best is not None:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best.items()})\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3857c336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Practice_PNGandJPG] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Practice_PNGandJPG\n",
      "[Practice_DICOM] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Practice_DICOM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3226101/3084858846.py:22: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  Image.fromarray(ensure_uint8(np.array(im)), mode=\"L\").save(out_dir / (png_path.stem + \".jpg\"), quality=95)\n",
      "/tmp/ipykernel_3226101/3084858846.py:27: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  Image.fromarray(ensure_uint8(np.array(im)), mode=\"L\").save(out_dir / (jpg_path.stem + \".png\"))\n",
      "/tmp/ipykernel_3226101/3084858846.py:37: DeprecationWarning: 'mode' parameter is deprecated and will be removed in Pillow 13 (2026-10-15)\n",
      "  Image.fromarray(arr, mode=\"L\").save(out_dir / (dcm_path.stem + \".png\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 0] PNG=15 JPG=15 DICOM=10\n",
      "  Example PNG->JPG: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Practice_PNGandJPG/_converted/JPCLN001.jpg\n",
      "  Example JPG->PNG: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Practice_PNGandJPG/_converted/JPCLN001.png\n",
      "  Example DCM->PNG: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Practice_DICOM/_png/JPCLN001.png\n"
     ]
    }
   ],
   "source": [
    "#0 Read and Write\n",
    "\n",
    "PRACTICE = {\n",
    "    \"Practice_PNGandJPG\": \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/Practice_PNGandJPG.zip\",\n",
    "    \"Practice_DICOM\":     \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/Practice_DICOM.zip\",\n",
    "}\n",
    "prac_img_dir = download_if_needed(\"Practice_PNGandJPG\", PRACTICE[\"Practice_PNGandJPG\"])\n",
    "prac_dcm_dir = download_if_needed(\"Practice_DICOM\",     PRACTICE[\"Practice_DICOM\"])\n",
    "\n",
    "def ensure_uint8(arr):\n",
    "    arr = np.asarray(arr).astype(np.float32)\n",
    "    lo, hi = np.percentile(arr, [0.5, 99.5])\n",
    "    if hi <= lo:\n",
    "        lo, hi = arr.min(), arr.max()\n",
    "    arr = np.clip(arr, lo, hi)\n",
    "    arr = (255.0 * (arr - lo) / max(hi - lo, 1e-6)).round().astype(np.uint8)\n",
    "    return arr\n",
    "\n",
    "def png_to_jpg(png_path, out_dir):\n",
    "    im = Image.open(png_path).convert(\"L\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(ensure_uint8(np.array(im)), mode=\"L\").save(out_dir / (png_path.stem + \".jpg\"), quality=95)\n",
    "\n",
    "def jpg_to_png(jpg_path, out_dir):\n",
    "    im = Image.open(jpg_path).convert(\"L\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(ensure_uint8(np.array(im)), mode=\"L\").save(out_dir / (jpg_path.stem + \".png\"))\n",
    "\n",
    "def dcm_to_png(dcm_path, out_dir):\n",
    "    try:\n",
    "        import pydicom\n",
    "    except ImportError as e:\n",
    "        raise ImportError(\"Please install pydicom first: pip install pydicom\") from e\n",
    "    ds = pydicom.dcmread(str(dcm_path))\n",
    "    arr = ensure_uint8(ds.pixel_array)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    Image.fromarray(arr, mode=\"L\").save(out_dir / (dcm_path.stem + \".png\"))\n",
    "\n",
    "# 3) Enumerate & convert a few examples (non-destructive; saves to _converted/_png)\n",
    "pngs = sorted(prac_img_dir.rglob(\"*.png\"))\n",
    "jpgs = sorted(prac_img_dir.rglob(\"*.jpg\"))\n",
    "dcms = sorted(prac_dcm_dir.rglob(\"*.dcm\"))\n",
    "\n",
    "out_img_conv = prac_img_dir / \"_converted\"   # stays inside Practice_PNGandJPG\n",
    "out_dcm_png  = prac_dcm_dir / \"_png\"         # stays inside Practice_DICOM\n",
    "\n",
    "MAX_SAVE = 5  # keep quick; change as needed\n",
    "for p in pngs[:MAX_SAVE]:\n",
    "    png_to_jpg(p, out_img_conv)\n",
    "for j in jpgs[:MAX_SAVE]:\n",
    "    jpg_to_png(j, out_img_conv)\n",
    "for d in dcms[:MAX_SAVE]:\n",
    "    dcm_to_png(d, out_dcm_png)\n",
    "\n",
    "print(f\"[Task 0] PNG={len(pngs)} JPG={len(jpgs)} DICOM={len(dcms)}\")\n",
    "if pngs: print(\"  Example PNG->JPG:\", out_img_conv / (pngs[0].stem + \".jpg\"))\n",
    "if jpgs: print(\"  Example JPG->PNG:\", out_img_conv / (jpgs[0].stem + \".png\"))\n",
    "if dcms: print(\"  Example DCM->PNG:\", out_dcm_png  / (dcms[0].stem + \".png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd74f6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Directions01] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Directions01\n",
      "Classes (Task 1): ['down', 'left', 'right', 'up']\n",
      "E01 | train 0.8438/0.747 | val 0.7818/0.750 | 17.6s\n",
      "E02 | train 0.3540/0.947 | val 0.3092/0.975 | 1.7s\n",
      "E03 | train 0.2174/0.985 | val 0.1689/0.975 | 1.7s\n",
      "E04 | train 0.1689/0.979 | val 0.1226/1.000 | 1.8s\n",
      "E05 | train 0.1273/0.993 | val 0.1135/0.975 | 1.7s\n",
      "[Task 1] Test loss: 0.1226 | Test acc: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Directions01\n",
    "DATASETS = {\n",
    "    \"Directions01\": \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Directions01.zip\",\n",
    "}\n",
    "\n",
    "directions_dir = download_if_needed(\"Directions01\", DATASETS[\"Directions01\"])\n",
    "train_dir_1 = directions_dir / \"train\"   # expects subfolders: Up, Down, Left, Right\n",
    "test_dir_1  = directions_dir / \"test\"\n",
    "\n",
    "train_ds_1 = datasets.ImageFolder(train_dir_1, transform=cls_tfms)\n",
    "test_ds_1  = datasets.ImageFolder(test_dir_1,  transform=cls_tfms)\n",
    "print(\"Classes (Task 1):\", train_ds_1.classes)\n",
    "\n",
    "num_workers = 2\n",
    "train_ld_1 = DataLoader(train_ds_1, batch_size=32, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
    "test_ld_1  = DataLoader(test_ds_1,  batch_size=64, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model_1 = build_resnet18(num_classes=4, freeze_backbone=True)\n",
    "model_1 = fit(model_1, train_ld_1, test_ld_1, epochs=5, lr=1e-3)\n",
    "\n",
    "test_loss_1, test_acc_1 = run_epoch(model_1, test_ld_1, nn.CrossEntropyLoss(), optimizer=None)\n",
    "print(f\"[Task 1] Test loss: {test_loss_1:.4f} | Test acc: {test_acc_1:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba3515d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gender01] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Gender01\n",
      "Classes (Task 2): ['female', 'male']\n",
      "E01 | train 0.6466/0.610 | val 0.6250/0.667 | 7.0s\n",
      "E02 | train 0.5214/0.831 | val 0.7320/0.473 | 0.8s\n",
      "E03 | train 0.4514/0.805 | val 0.5267/0.806 | 1.8s\n",
      "E04 | train 0.3908/0.896 | val 0.5959/0.634 | 0.7s\n",
      "E05 | train 0.3824/0.870 | val 0.4641/0.882 | 2.2s\n",
      "E06 | train 0.2969/0.955 | val 0.4796/0.806 | 1.5s\n",
      "[Task 2] Test loss: 0.4641 | Test acc: 0.882\n",
      "\n",
      "[Task 2] report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      female      0.897     0.833     0.864        42\n",
      "        male      0.870     0.922     0.895        51\n",
      "\n",
      "    accuracy                          0.882        93\n",
      "   macro avg      0.884     0.877     0.880        93\n",
      "weighted avg      0.883     0.882     0.881        93\n",
      "\n",
      "[Task 2] confusion:\n",
      " [[35  7]\n",
      " [ 4 47]]\n"
     ]
    }
   ],
   "source": [
    "# Gender01\n",
    "DATASETS[\"Gender01\"] = \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Gender01.zip\"\n",
    "\n",
    "gender_dir = download_if_needed(\"Gender01\", DATASETS[\"Gender01\"])\n",
    "train_dir_2 = gender_dir / \"train\"   # subfolders: female, male\n",
    "test_dir_2  = gender_dir / \"test\"\n",
    "\n",
    "train_ds_2 = datasets.ImageFolder(train_dir_2, transform=cls_tfms)\n",
    "test_ds_2  = datasets.ImageFolder(test_dir_2,  transform=cls_tfms)\n",
    "print(\"Classes (Task 2):\", train_ds_2.classes)\n",
    "\n",
    "train_ld_2 = DataLoader(train_ds_2, batch_size=16, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
    "test_ld_2  = DataLoader(test_ds_2,  batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "model_2 = build_resnet18(num_classes=2, freeze_backbone=True)\n",
    "model_2 = fit(model_2, train_ld_2, test_ld_2, epochs=6, lr=1e-3)\n",
    "\n",
    "test_loss_2, test_acc_2 = run_epoch(model_2, test_ld_2, nn.CrossEntropyLoss(), optimizer=None)\n",
    "print(f\"[Task 2] Test loss: {test_loss_2:.4f} | Test acc: {test_acc_2:.3f}\")\n",
    "\n",
    "\n",
    "def collect_preds(model, loader):\n",
    "    model.eval(); P=[]; T=[]\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            P += model(x).argmax(1).cpu().tolist()\n",
    "            T += y.tolist()\n",
    "    return np.array(P), np.array(T)\n",
    "\n",
    "p2, t2 = collect_preds(model_2, test_ld_2)\n",
    "print(\"\\n[Task 2] report:\\n\", classification_report(t2, p2, target_names=train_ds_2.classes, digits=3))\n",
    "print(\"[Task 2] confusion:\\n\", confusion_matrix(t2, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac77a087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gender01] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Gender01\n",
      "Classes (Task 2): ['female', 'male']\n",
      "[T2-w] E01 | train 0.7934/0.506 | val 0.6660/0.538\n",
      "[T2-w] E02 | train 0.6440/0.584 | val 0.8680/0.441\n",
      "[T2-w] E03 | train 0.5494/0.779 | val 0.6580/0.581\n",
      "[T2-w] E04 | train 0.4762/0.851 | val 0.6312/0.634\n",
      "[T2-w] E05 | train 0.3994/0.903 | val 0.5311/0.731\n",
      "[T2-w] E06 | train 0.3574/0.922 | val 0.4839/0.860\n",
      "[Task 2] Test loss: 0.4839 | Test acc: 0.860\n",
      "\n",
      "[Task 2] report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      female      0.837     0.857     0.847        42\n",
      "        male      0.880     0.863     0.871        51\n",
      "\n",
      "    accuracy                          0.860        93\n",
      "   macro avg      0.859     0.860     0.859        93\n",
      "weighted avg      0.861     0.860     0.860        93\n",
      "\n",
      "[Task 2] confusion:\n",
      " [[36  6]\n",
      " [ 7 44]]\n",
      "[T2-w] E01 | train 0.2500/0.935 | val 0.3368/0.882\n",
      "[T2-w] E02 | train 0.0978/0.974 | val 0.3282/0.882\n"
     ]
    }
   ],
   "source": [
    "# 1) Dataset & loaders\n",
    "DATASETS[\"Gender01\"] = \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Gender01.zip\"\n",
    "gender_dir = download_if_needed(\"Gender01\", DATASETS[\"Gender01\"])\n",
    "train_dir_2 = gender_dir / \"train\"\n",
    "test_dir_2  = gender_dir / \"test\"\n",
    "\n",
    "train_ds_2 = datasets.ImageFolder(train_dir_2, transform=cls_tfms)\n",
    "test_ds_2  = datasets.ImageFolder(test_dir_2,  transform=cls_tfms)\n",
    "print(\"Classes (Task 2):\", train_ds_2.classes)\n",
    "\n",
    "num_workers = 2\n",
    "train_ld_2 = DataLoader(train_ds_2, batch_size=16, shuffle=True,  num_workers=num_workers, pin_memory=True)\n",
    "test_ld_2  = DataLoader(test_ds_2,  batch_size=32, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# 2) Class weights (inverse frequency, normalized to mean=1)\n",
    "cnt = Counter(train_ds_2.targets)\n",
    "num_classes = len(train_ds_2.classes)\n",
    "counts = torch.tensor([cnt.get(i, 0) for i in range(num_classes)], dtype=torch.float32)\n",
    "cls_weights = counts.sum() / (counts + 1e-6)\n",
    "cls_weights = cls_weights / cls_weights.mean()\n",
    "\n",
    "# 3) Weighted fit helper\n",
    "def fit_weighted(model, train_loader, val_loader, class_weights, epochs=6, lr=1e-3):\n",
    "    crit = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    best_acc, best = -1.0, None\n",
    "    for ep in range(1, epochs+1):\n",
    "        tr_loss, tr_acc = run_epoch(model, train_loader, crit, optimizer=opt)\n",
    "        va_loss, va_acc = run_epoch(model, val_loader,   crit, optimizer=None)\n",
    "        if va_acc > best_acc:\n",
    "            best_acc, best = va_acc, {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "        print(f\"[T2-w] E{ep:02d} | train {tr_loss:.4f}/{tr_acc:.3f} | val {va_loss:.4f}/{va_acc:.3f}\")\n",
    "    if best is not None:\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in best.items()})\n",
    "    return model\n",
    "\n",
    "# 4) Train (head-only) USING ImageNet weights variable 'weights' (ResNet18_Weights.DEFAULT)\n",
    "model_2 = build_resnet18(num_classes=2, freeze_backbone=True)\n",
    "model_2 = fit_weighted(model_2, train_ld_2, test_ld_2, class_weights=cls_weights, epochs=6, lr=1e-3)\n",
    "\n",
    "# 5) Test accuracy (quick)\n",
    "test_loss_2, test_acc_2 = run_epoch(model_2, test_ld_2, nn.CrossEntropyLoss(weight=cls_weights.to(DEVICE)), optimizer=None)\n",
    "print(f\"[Task 2] Test loss: {test_loss_2:.4f} | Test acc: {test_acc_2:.3f}\")\n",
    "\n",
    "# 6) Detailed report + confusion matrix\n",
    "def collect_preds(model, loader):\n",
    "    model.eval(); P=[]; T=[]\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            P += model(x).argmax(1).cpu().tolist()\n",
    "            T += y.tolist()\n",
    "    return np.array(P), np.array(T)\n",
    "\n",
    "p2, t2 = collect_preds(model_2, test_ld_2)\n",
    "print(\"\\n[Task 2] report:\\n\", classification_report(t2, p2, target_names=train_ds_2.classes, digits=3))\n",
    "print(\"[Task 2] confusion:\\n\", confusion_matrix(t2, p2))\n",
    "\n",
    "# 7) (Optional) brief unfreeze FT @ low LR\n",
    "for p in model_2.parameters(): p.requires_grad = True\n",
    "model_2 = fit_weighted(model_2, train_ld_2, test_ld_2, class_weights=cls_weights, epochs=2, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "540ea7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XPAge01_RGB] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/XPAge01_RGB\n",
      "[Age] CSV: /scratch/hdharmen/ASU/CSE 507/data/jsrt/XPAge01_RGB/XP/testdata.csv\n",
      "[Age] Columns: ['filenames', 'age']\n",
      "[Age] Stratified split -> Train=132, Test=33 | mean=60.03, std=13.68\n"
     ]
    }
   ],
   "source": [
    "# --- Task 3: Age estimation (XPAge01_RGB) ---\n",
    "\n",
    "# 1) Download & find CSV\n",
    "age_dir = download_if_needed(\n",
    "    \"XPAge01_RGB\",\n",
    "    \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/XPAge01_RGB.zip\"\n",
    ")\n",
    "\n",
    "# Robust CSV finder (defines only if missing)\n",
    "if 'find_first_csv' not in globals():\n",
    "    def find_first_csv(root: Path):\n",
    "        cands = list(root.rglob(\"*.csv\"))\n",
    "        if not cands:\n",
    "            raise FileNotFoundError(f\"No CSV found under {root}\")\n",
    "        pref = [p for p in cands if \"age\" in p.name.lower() or \"xpage\" in p.name.lower()]\n",
    "        return pref[0] if pref else cands[0]\n",
    "\n",
    "csv_path = find_first_csv(age_dir)\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"[Age] CSV:\", csv_path)\n",
    "print(\"[Age] Columns:\", list(df.columns))\n",
    "\n",
    "# 2) Auto-detect filename/age columns\n",
    "fname_col = next((c for c in df.columns if c.lower().startswith(\"file\")), df.columns[0])\n",
    "age_col   = next((c for c in df.columns if \"age\" in c.lower()), df.columns[-1])\n",
    "\n",
    "# 3) Image resolver (works even if images are in nested folders)\n",
    "def resolve_img(root: Path, fname: str) -> Path:\n",
    "    p = root / fname\n",
    "    if p.exists(): return p\n",
    "    hits = list(root.rglob(fname))\n",
    "    if hits: return hits[0]\n",
    "    raise FileNotFoundError(f\"Image {fname} not found under {root}\")\n",
    "\n",
    "# 4) Dataset (uses same normalization as your classifiers)\n",
    "age_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    preprocess,  # from your setup cell\n",
    "])\n",
    "\n",
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, table: pd.DataFrame, img_root: Path, fname_col: str, age_col: str, transform=None):\n",
    "        self.table = table.reset_index(drop=True)\n",
    "        self.img_root = img_root\n",
    "        self.fname_col = fname_col\n",
    "        self.age_col = age_col\n",
    "        self.transform = transform\n",
    "    def __len__(self): \n",
    "        return len(self.table)\n",
    "    def __getitem__(self, i):\n",
    "        row = self.table.iloc[i]\n",
    "        img = Image.open(resolve_img(self.img_root, str(row[self.fname_col]))).convert(\"RGB\")\n",
    "        if self.transform: \n",
    "            img = self.transform(img)\n",
    "        age = torch.tensor([float(row[self.age_col])], dtype=torch.float32)  # [1]\n",
    "        return img, age\n",
    "\n",
    "\n",
    "# 5) Split + loaders (STRATIFIED by age bins) + light aug on TRAIN only\n",
    "\n",
    "from torchvision import transforms as T\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratify by coarse age bins to reduce train/test drift\n",
    "bins = pd.cut(df[age_col], bins=[16, 30, 40, 50, 60, 70, 80, 90], labels=False, include_lowest=True)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=bins, random_state=SEED)\n",
    "\n",
    "# Light, label-safe aug for train; clean eval for test\n",
    "age_train_tfms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    T.RandomRotation(degrees=3),\n",
    "    T.ColorJitter(brightness=0.05, contrast=0.05),\n",
    "    preprocess,                       # your ImageNet pipeline\n",
    "])\n",
    "age_eval_tfms = T.Compose([\n",
    "    T.Grayscale(num_output_channels=3),\n",
    "    preprocess,\n",
    "])\n",
    "\n",
    "ds_tr_3 = AgeDataset(train_df, age_dir, fname_col, age_col, transform=age_train_tfms)\n",
    "ds_te_3 = AgeDataset(test_df,  age_dir, fname_col, age_col, transform=age_eval_tfms)\n",
    "\n",
    "ld_tr_3 = DataLoader(ds_tr_3, batch_size=8, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "ld_te_3 = DataLoader(ds_te_3, batch_size=8, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Recompute z-score stats on TRAIN ONLY\n",
    "age_mean = float(train_df[age_col].mean())\n",
    "age_std  = float(train_df[age_col].std() + 1e-6)\n",
    "print(f\"[Age] Stratified split -> Train={len(ds_tr_3)}, Test={len(ds_te_3)} | mean={age_mean:.2f}, std={age_std:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24bc18bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 3] E01 | train(z) 0.4167 | val RMSE 14.85 yrs\n",
      "[Task 3] E02 | train(z) 0.3308 | val RMSE 13.18 yrs\n",
      "[Task 3] E03 | train(z) 0.3704 | val RMSE 11.98 yrs\n",
      "[Task 3] E04 | train(z) 0.3666 | val RMSE 12.10 yrs\n",
      "[Task 3] E05 | train(z) 0.3039 | val RMSE 11.37 yrs\n",
      "[Task 3] E06 | train(z) 0.2891 | val RMSE 13.14 yrs\n",
      "[Task 3] E07 | train(z) 0.3722 | val RMSE 14.53 yrs\n",
      "[Task 3] E08 | train(z) 0.3133 | val RMSE 11.68 yrs\n",
      "[Task 3] E01 | train(z) 0.2681 | val RMSE 10.63 yrs\n",
      "[Task 3] E02 | train(z) 0.2350 | val RMSE 10.03 yrs\n",
      "[Task 3] E03 | train(z) 0.1700 | val RMSE 10.02 yrs\n",
      "[Task 3] E04 | train(z) 0.1109 | val RMSE 9.58 yrs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Compute z-score params on TRAIN ONLY\n",
    "age_mean = float(train_df[age_col].mean())\n",
    "age_std  = float(train_df[age_col].std() + 1e-6)\n",
    "\n",
    "USE_HUBER = True\n",
    "crit_reg = nn.SmoothL1Loss(beta=1.0) if USE_HUBER else nn.MSELoss()\n",
    "\n",
    "def train_regression_z(model, train_loader, val_loader, epochs=5, lr=1e-4):\n",
    "    \"\"\"Train regression in standardized space; report val RMSE in years.\"\"\"\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); tr=0\n",
    "        for x,y in train_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = ((y.to(DEVICE).squeeze(1) - age_mean) / age_std)  # z-score targets\n",
    "            opt.zero_grad()\n",
    "            pred_z = model(x).squeeze(1)\n",
    "            loss = crit_reg(pred_z, y)\n",
    "            loss.backward(); opt.step()\n",
    "            tr += loss.item() * x.size(0)\n",
    "        # Validation in YEARS for readability\n",
    "        model.eval(); se=0; n=0\n",
    "        with torch.no_grad():\n",
    "            for x,y in val_loader:\n",
    "                x = x.to(DEVICE); y = y.squeeze(1).to(DEVICE)\n",
    "                pred_years = model(x).squeeze(1) * age_std + age_mean\n",
    "                se += ((pred_years - y)**2).sum().item()\n",
    "                n  += x.size(0)\n",
    "        print(f\"[Task 3] E{ep:02d} | train(z) {tr/len(train_loader.dataset):.4f} | val RMSE {np.sqrt(se/n):.2f} yrs\")\n",
    "\n",
    "# Build ResNet18 with a slightly stronger MLP head\n",
    "def build_resnet18_regression():\n",
    "    m = resnet18(weights=weights)\n",
    "    for p in m.parameters():\n",
    "        p.requires_grad = False\n",
    "    in_feats = m.fc.in_features\n",
    "    m.fc = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(in_feats, 256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 1)\n",
    "    )\n",
    "    return m.to(DEVICE)\n",
    "\n",
    "model_3 = build_resnet18_regression()\n",
    "\n",
    "# Stage 1: head-only (longer, higher LR since only head learns)\n",
    "train_regression_z(model_3, ld_tr_3, ld_te_3, epochs=8, lr=1e-3)\n",
    "\n",
    "# Stage 2: unfreeze only layer4 + head for a short, low-LR fine-tune\n",
    "for name, p in model_3.named_parameters():\n",
    "    p.requires_grad = (name.startswith(\"layer4\") or name.startswith(\"fc\"))\n",
    "train_regression_z(model_3, ld_tr_3, ld_te_3, epochs=4, lr=3e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1030c94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 3] Test MAE: 6.86 yrs | RMSE: 9.58 yrs\n",
      "[Task 3-TTA] Test MAE: 6.89 yrs | RMSE: 9.22 yrs\n",
      "\n",
      "[Task 3] Summary\n",
      "  Test MAE  : 6.86 yrs\n",
      "  Test RMSE : 9.58 yrs\n",
      "  R^2       : 0.501\n",
      "  Pearson r : 0.709\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 3] Wrote predictions to: /scratch/hdharmen/ASU/CSE 507/data/jsrt/age_predictions_test.csv\n"
     ]
    }
   ],
   "source": [
    "def evaluate_age_years(model, loader, clamp_to=(16, 89)):\n",
    "    model.eval(); preds=[]; gts=[]\n",
    "    lo, hi = clamp_to\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            p_years = (model(x).squeeze(1) * age_std + age_mean).cpu().numpy()\n",
    "            preds.extend(p_years.tolist()); gts.extend(y.squeeze(1).numpy().tolist())\n",
    "    preds = np.array(preds); gts = np.array(gts)\n",
    "    if clamp_to is not None:\n",
    "        preds = np.clip(preds, lo, hi)  # clamp only for reporting\n",
    "    mae  = np.abs(preds - gts).mean()\n",
    "    rmse = np.sqrt(((preds - gts)**2).mean())\n",
    "    return mae, rmse\n",
    "\n",
    "mae, rmse = evaluate_age_years(model_3, ld_te_3)\n",
    "print(f\"[Task 3] Test MAE: {mae:.2f} yrs | RMSE: {rmse:.2f} yrs\")\n",
    "# --- Optional TTA eval (original + horizontal flip) ---\n",
    "def evaluate_age_years_tta(model, loader, clamp_to=(16, 89)):\n",
    "    model.eval(); preds=[]; gts=[]\n",
    "    lo, hi = clamp_to if clamp_to else (None, None)\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            p1 = model(x).squeeze(1)\n",
    "            p2 = model(torch.flip(x, dims=[3])).squeeze(1)  # H-flip\n",
    "            p = 0.5 * (p1 + p2)\n",
    "            p_years = (p * age_std + age_mean).cpu().numpy()\n",
    "            preds.extend(p_years.tolist())\n",
    "            gts.extend(y.squeeze(1).numpy().tolist())\n",
    "    preds = np.array(preds); gts = np.array(gts)\n",
    "    if clamp_to is not None:\n",
    "        preds = np.clip(preds, lo, hi)\n",
    "    mae  = np.abs(preds - gts).mean()\n",
    "    rmse = np.sqrt(((preds - gts)**2).mean())\n",
    "    return mae, rmse\n",
    "\n",
    "mae_tta, rmse_tta = evaluate_age_years_tta(model_3, ld_te_3)\n",
    "print(f\"[Task 3-TTA] Test MAE: {mae_tta:.2f} yrs | RMSE: {rmse_tta:.2f} yrs\")\n",
    "\n",
    "# --- Pretty results: R^2, Pearson r, plots, CSV ---\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "def collect_preds_years(model, loader):\n",
    "    model.eval(); preds=[]; gts=[]\n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            p_years = (model(x).squeeze(1) * age_std + age_mean).cpu().numpy()\n",
    "            preds.extend(p_years.tolist()); gts.extend(y.squeeze(1).numpy().tolist())\n",
    "    return np.array(preds), np.array(gts)\n",
    "\n",
    "preds, gts = collect_preds_years(model_3, ld_te_3)\n",
    "preds_clip = np.clip(preds, 16, 89)\n",
    "\n",
    "mae  = np.abs(preds_clip - gts).mean()\n",
    "rmse = np.sqrt(((preds_clip - gts)**2).mean())\n",
    "r2   = r2_score(gts, preds_clip)\n",
    "corr = np.corrcoef(gts, preds_clip)[0,1]\n",
    "\n",
    "print(\"\\n[Task 3] Summary\")\n",
    "print(f\"  Test MAE  : {mae:.2f} yrs\")\n",
    "print(f\"  Test RMSE : {rmse:.2f} yrs\")\n",
    "print(f\"  R^2       : {r2:.3f}\")\n",
    "print(f\"  Pearson r : {corr:.3f}\")\n",
    "\n",
    "# Scatter: predicted vs ground-truth\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(gts, preds_clip, s=18)\n",
    "plt.plot([16, 89], [16, 89])\n",
    "plt.xlabel(\"Ground-truth age (years)\")\n",
    "plt.ylabel(\"Predicted age (years)\")\n",
    "plt.title(\"Task 4: Predicted vs Ground-truth\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Error histogram\n",
    "plt.figure(figsize=(5,4))\n",
    "errors = preds_clip - gts\n",
    "plt.hist(errors, bins=15)\n",
    "plt.xlabel(\"Prediction error (years)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Task 4: Error distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save predictions CSV\n",
    "pred_csv = BASE_DIR / \"age_predictions_test.csv\"\n",
    "with open(pred_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"index\",\"y_true\",\"y_pred\",\"error\"])\n",
    "    for i,(t,p) in enumerate(zip(gts, preds_clip)):\n",
    "        w.writerow([i, float(t), float(p), float(p-t)])\n",
    "print(f\"[Task 3] Wrote predictions to: {pred_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b780df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Segmentation01] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation01\n",
      "[Segmentation02] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02\n",
      "[Seg1] /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation01\n",
      "[Seg2] /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02\n"
     ]
    }
   ],
   "source": [
    "# --- Segmentation datasets (Task 46) ---\n",
    "DATASETS.update({\n",
    "    \"Segmentation01\": \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Segmentation01.zip\",  # lungs\n",
    "    \"Segmentation02\": \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2019/07/segmentation02.zip\",  # organs (instances/bboxes)\n",
    "})\n",
    "\n",
    "seg1_dir = download_if_needed(\"Segmentation01\", DATASETS[\"Segmentation01\"])\n",
    "seg2_dir = download_if_needed(\"Segmentation02\", DATASETS[\"Segmentation02\"])  # needed for Tasks 56\n",
    "\n",
    "print(\"[Seg1]\", seg1_dir)\n",
    "print(\"[Seg2]\", seg2_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18a6dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- ResNet-18  simple FCN-32s head (returns logits [N,C,H,W]) ---\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def build_fcn_resnet18(num_classes: int, freeze_backbone: bool = True):\n",
    "    m = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "    # backbone up to layer4\n",
    "    backbone = torch.nn.Sequential(\n",
    "        m.conv1, m.bn1, m.relu, m.maxpool,\n",
    "        m.layer1, m.layer2, m.layer3, m.layer4\n",
    "    )\n",
    "    if freeze_backbone:\n",
    "        for p in backbone.parameters(): p.requires_grad = False\n",
    "    head = torch.nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "    model = torch.nn.Module()\n",
    "    model.backbone = backbone\n",
    "    model.head = head\n",
    "    def forward(x):\n",
    "        feat = model.backbone(x)             # [N,512,H/32,W/32]\n",
    "        logits = model.head(feat)            # [N,C,H/32,W/32]\n",
    "        logits = F.interpolate(logits, size=x.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        return logits\n",
    "    model.forward = forward\n",
    "    return model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d0ae921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper: try to find \"images\" and \"masks\" dirs under train/ and test/ ---\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "def _find_subdir(root, part, key_words):\n",
    "    base = root / part\n",
    "    if not base.exists(): return None\n",
    "    # prefer explicit subdirs\n",
    "    for k in key_words:\n",
    "        c = next((p for p in base.rglob(\"*\") if p.is_dir() and k in p.name.lower()), None)\n",
    "        if c: return c\n",
    "    # fallback: the part itself has images/masks mixed -> return part\n",
    "    return base\n",
    "\n",
    "def _pair_by_stem(img_dir, mask_dir, img_exts=(\".png\",\".jpg\",\".jpeg\",\".bmp\"), mask_exts=(\".png\",\".bmp\")):\n",
    "    images = [p for p in img_dir.rglob(\"*\") if p.suffix.lower() in img_exts]\n",
    "    mask_lookup = {}\n",
    "    for q in mask_dir.rglob(\"*\"):\n",
    "        if q.suffix.lower() in mask_exts:\n",
    "            mask_lookup[q.stem.lower()] = q\n",
    "    pairs = []\n",
    "    for ip in images:\n",
    "        mp = mask_lookup.get(ip.stem.lower())\n",
    "        if mp: pairs.append((ip, mp))\n",
    "    if not pairs:\n",
    "        raise FileNotFoundError(f\"No img/mask pairs under {img_dir}  {mask_dir}\")\n",
    "    return pairs\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "class LungSegDataset(Dataset):\n",
    "    def __init__(self, pairs, size=384, augment=False):\n",
    "        self.pairs = pairs\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "        img  = Image.open(ip).convert(\"L\")\n",
    "        mask = Image.open(mp)\n",
    "        # strip constant borders (JSRT corner tags / film borders)\n",
    "        a = np.array(img, dtype=np.uint8)\n",
    "        # detect top/bottom rows that are near-constant\n",
    "        def _trim(v, thr=3):\n",
    "            i0, i1 = 0, v.shape[0]-1\n",
    "            while i0 < i1 and v[i0].max()-v[i0].min() <= thr: i0 += 1\n",
    "            while i1 > i0 and v[i1].max()-v[i1].min() <= thr: i1 -= 1\n",
    "            return i0, i1+1\n",
    "        r0, r1 = _trim(a); c0, c1 = _trim(a.T)\n",
    "        a = a[r0:r1, c0:c1]; img = Image.fromarray(a)\n",
    "        mask = mask.crop((c0, r0, c1, r1))\n",
    "\n",
    "\n",
    "        # resize first (keep same for img & mask)\n",
    "        img  = TF.resize(img,  (self.size, self.size), InterpolationMode.BILINEAR)\n",
    "        mask = TF.resize(mask, (self.size, self.size), InterpolationMode.NEAREST)\n",
    "\n",
    "        # optional simple aug\n",
    "        if self.augment and random.random() < 0.5:\n",
    "            img  = TF.hflip(img)\n",
    "            mask = TF.hflip(mask)\n",
    "\n",
    "        # to tensors\n",
    "        x1 = TF.to_tensor(img)[0:1]\n",
    "        x1 = (x1 - x1.mean()) / (x1.std() + 1e-6)\n",
    "        x  = x1.repeat(3,1,1)\n",
    "\n",
    "        # some masks in JSRT are 0 / 255; threshold at 128\n",
    "        m = (np.array(mask, dtype=np.uint8) >= 128).astype(np.int64)\n",
    "        m = torch.from_numpy(m)\n",
    "        return x, m\n",
    "\n",
    "class OrganSegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Multi-class organ masks (instance-by-class, e.g., background=0, left_lung=1, right_lung=2, heart=3, ...).\n",
    "    Assumes mask pixel values already encode classes; if they are 0/255 per organ, merge beforehand or adjust here.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs, size=256):\n",
    "        self.pairs = pairs\n",
    "        self.size = size\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "        img  = Image.open(ip).convert(\"L\")\n",
    "        mask = Image.open(mp)  # keep as P/L; unique values = classes\n",
    "        img  = TF.resize(img,  (self.size, self.size), InterpolationMode.BILINEAR)\n",
    "        mask = TF.resize(mask, (self.size, self.size), InterpolationMode.NEAREST)\n",
    "        x = TF.to_tensor(img).repeat(3,1,1)\n",
    "        x = TF.normalize(x, IMAGENET_MEAN, IMAGENET_STD)\n",
    "        m = torch.from_numpy(np.array(mask, dtype=np.int64))\n",
    "        # If mask is {0,255} for each organ channel separately, change this to your merge logic.\n",
    "        return x, m\n",
    "    \n",
    "# Put this near your other helpers\n",
    "import numpy as np\n",
    "from skimage import measure, morphology\n",
    "from scipy.ndimage import distance_transform_edt as dist_edt\n",
    "\n",
    "def semantic_to_instances_lr_heart(lbl2d, lung_id=1, heart_id=2):\n",
    "    \"\"\"\n",
    "    lbl2d: HxW semantic labels (0=bg, lung_id=lungs, heart_id=heart)\n",
    "    returns dict of binary masks: {'left_lung','right_lung','heart'}\n",
    "    \"\"\"\n",
    "    H, W = lbl2d.shape\n",
    "    out = {'left_lung': np.zeros((H,W), bool),\n",
    "           'right_lung': np.zeros((H,W), bool),\n",
    "           'heart': (lbl2d == heart_id)}\n",
    "\n",
    "    lungs = (lbl2d == lung_id)\n",
    "    if lungs.any():\n",
    "        # split connected components; keep two largest\n",
    "        labeled = measure.label(lungs)\n",
    "        props = sorted(measure.regionprops(labeled), key=lambda r: r.area, reverse=True)\n",
    "        if len(props) >= 2:\n",
    "            a, b = props[0].label, props[1].label\n",
    "            compA, compB = (labeled == a), (labeled == b)\n",
    "        else:\n",
    "            # single blob: split by watershed over distance to center seam\n",
    "            d = dist_edt(lungs)\n",
    "            # vertical seam prior: left vs right of image center\n",
    "            xgrid = np.tile(np.arange(W), (H,1))\n",
    "            left_prior  = (xgrid < W//2).astype(np.float32)\n",
    "            right_prior = 1.0 - left_prior\n",
    "            left_score  = d + left_prior*2.0\n",
    "            right_score = d + right_prior*2.0\n",
    "            compA = (left_score  >= right_score) & lungs\n",
    "            compB = (right_score >  left_score) & lungs\n",
    "\n",
    "        # decide which is left/right by centroid x\n",
    "        def centroid_x(mask):\n",
    "            ys, xs = np.nonzero(mask)\n",
    "            return xs.mean() if xs.size else 0\n",
    "        if centroid_x(compA) <= centroid_x(compB):\n",
    "            out['left_lung']  = morphology.remove_small_holes(compA, area_threshold=128)\n",
    "            out['right_lung'] = morphology.remove_small_holes(compB, area_threshold=128)\n",
    "        else:\n",
    "            out['left_lung']  = morphology.remove_small_holes(compB, area_threshold=128)\n",
    "            out['right_lung'] = morphology.remove_small_holes(compA, area_threshold=128)\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00930ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Segmentation training + mIoU ---\n",
    "def train_seg(model, train_loader, val_loader=None, epochs=15, lr=1e-3, num_classes=2):\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "    opt  = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); tot=0\n",
    "        for x,y in train_loader:\n",
    "            x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            logits = model(x)                  # [N,C,H,W]\n",
    "            loss = crit(logits, y.long())\n",
    "            loss.backward(); opt.step()\n",
    "            tot += loss.item()*x.size(0)\n",
    "        msg = f\"[Seg] E{ep:02d} train {tot/len(train_loader.dataset):.4f}\"\n",
    "        if val_loader:\n",
    "            mi = mean_iou_seg(model, val_loader, num_classes)\n",
    "            msg += f\" | mIoU {mi:.3f}\"\n",
    "        print(msg)\n",
    "\n",
    "def mean_iou_seg(model, loader, num_classes):\n",
    "    model.eval(); ious=[]; \n",
    "    with torch.no_grad():\n",
    "        for x,y in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            pred = model(x).argmax(1).cpu()   # [N,H,W]\n",
    "            y    = y.long()\n",
    "            for p,t in zip(pred, y):\n",
    "                scores=[]\n",
    "                for c in range(num_classes):\n",
    "                    P = (p==c); T = (t==c)\n",
    "                    inter = (P & T).sum().item()\n",
    "                    union = (P | T).sum().item()\n",
    "                    if union>0: scores.append(inter/union)\n",
    "                if scores: ious.append(sum(scores)/len(scores))\n",
    "    return (sum(ious)/len(ious)) if ious else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5275cde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth\" to /home/hdharmen/.cache/torch/hub/checkpoints/deeplabv3_resnet50_coco-cd0a2569.pth\n",
      "100%|| 161M/161M [00:01<00:00, 128MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DeepLab] E01 train 0.4140 | mIoU 0.801\n",
      "[DeepLab] E02 train 0.2185 | mIoU 0.839\n",
      "[DeepLab] E03 train 0.1780 | mIoU 0.843\n",
      "[DeepLab] E04 train 0.1547 | mIoU 0.867\n",
      "[DeepLab] E05 train 0.1468 | mIoU 0.862\n",
      "[DeepLab] E06 train 0.1310 | mIoU 0.890\n",
      "[DeepLab] E07 train 0.1197 | mIoU 0.907\n",
      "[DeepLab] E08 train 0.1073 | mIoU 0.913\n",
      "[DeepLab] E09 train 0.1019 | mIoU 0.920\n",
      "[DeepLab] E10 train 0.0969 | mIoU 0.913\n",
      "[DeepLab] E11 train 0.0951 | mIoU 0.920\n",
      "[DeepLab] E12 train 0.0882 | mIoU 0.888\n",
      "[DeepLab] E13 train 0.0711 | mIoU 0.948\n",
      "[DeepLab] E14 train 0.0640 | mIoU 0.948\n",
      "[DeepLab] E15 train 0.0593 | mIoU 0.956\n",
      "[DeepLab] E16 train 0.0552 | mIoU 0.949\n",
      "[DeepLab] E17 train 0.0520 | mIoU 0.959\n",
      "[DeepLab] E18 train 0.0487 | mIoU 0.959\n",
      "[DeepLab] E19 train 0.0469 | mIoU 0.956\n",
      "[DeepLab] E20 train 0.0436 | mIoU 0.962\n",
      "[DeepLab] E21 train 0.0413 | mIoU 0.963\n",
      "[DeepLab] E22 train 0.0398 | mIoU 0.965\n",
      "[DeepLab] E23 train 0.0383 | mIoU 0.964\n",
      "[DeepLab] E24 train 0.0372 | mIoU 0.966\n",
      "[DeepLab] E25 train 0.0361 | mIoU 0.964\n",
      "[DeepLab] E26 train 0.0354 | mIoU 0.966\n",
      "[DeepLab] E27 train 0.0336 | mIoU 0.965\n",
      "[DeepLab] E28 train 0.0343 | mIoU 0.970\n",
      "[DeepLab] E29 train 0.0321 | mIoU 0.968\n",
      "[DeepLab] E30 train 0.0318 | mIoU 0.968\n",
      "[DeepLab] E31 train 0.0301 | mIoU 0.970\n",
      "[DeepLab] E32 train 0.0289 | mIoU 0.969\n",
      "[DeepLab] E33 train 0.0295 | mIoU 0.972\n",
      "[DeepLab] E34 train 0.0283 | mIoU 0.967\n",
      "[DeepLab] E35 train 0.0293 | mIoU 0.969\n",
      "[DeepLab] E36 train 0.0270 | mIoU 0.972\n",
      "[DeepLab] E37 train 0.0259 | mIoU 0.974\n",
      "[DeepLab] E38 train 0.0251 | mIoU 0.973\n",
      "[DeepLab] E39 train 0.0248 | mIoU 0.974\n",
      "[DeepLab] E40 train 0.0244 | mIoU 0.974\n",
      "[Task 4] Test mIoU (post-processed): 0.974\n"
     ]
    }
   ],
   "source": [
    "# ===================== Task 4: Better lung segmentation (DeepLabV3-R50) =====================\n",
    "# - Paired augmentations (geom) + image-only intensity jitter\n",
    "# - DeepLabV3-ResNet50 (COCO-pretrained), binary 1-logit head\n",
    "# - Loss: 0.5 * BCEWithLogits + 0.5 * Dice\n",
    "# - Post-process: keep two largest components + fill holes\n",
    "# - mIoU computed on post-processed predictions\n",
    "# ============================================================================================\n",
    "\n",
    "import random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50, DeepLabV3_ResNet50_Weights\n",
    "\n",
    "# ---- device ----\n",
    "if \"DEVICE\" not in globals():\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- imagenet normalization ----\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# ------------------------------- Dataset with paired augments --------------------------------\n",
    "class LungSegDataset(Dataset):\n",
    "    def __init__(self, pairs, size=512, augment=False):\n",
    "        self.pairs = pairs\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self): return len(self.pairs)\n",
    "\n",
    "    def _paired_affine(self, img, mask):\n",
    "        angle = random.uniform(-7, 7)\n",
    "        translate = (random.uniform(-0.05, 0.05)*img.width,\n",
    "                     random.uniform(-0.05, 0.05)*img.height)\n",
    "        scale = random.uniform(0.92, 1.08)\n",
    "        shear = 0.0\n",
    "        img  = TF.affine(img, angle=angle, translate=translate, scale=scale, shear=shear,\n",
    "                         interpolation=InterpolationMode.BILINEAR)\n",
    "        mask = TF.affine(mask, angle=angle, translate=translate, scale=scale, shear=shear,\n",
    "                         interpolation=InterpolationMode.NEAREST)\n",
    "        return img, mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "        img  = Image.open(ip).convert(\"L\")\n",
    "        mask = Image.open(mp)\n",
    "\n",
    "        # resize first (consistent for both)\n",
    "        img  = TF.resize(img,  (self.size, self.size), InterpolationMode.BILINEAR)\n",
    "        mask = TF.resize(mask, (self.size, self.size), InterpolationMode.NEAREST)\n",
    "\n",
    "        if self.augment:\n",
    "            if random.random() < 0.5:\n",
    "                img  = TF.hflip(img); mask = TF.hflip(mask)\n",
    "            if random.random() < 0.8:\n",
    "                img, mask = self._paired_affine(img, mask)\n",
    "            # image-only intensity jitter\n",
    "            if random.random() < 0.5:\n",
    "                img = TF.adjust_gamma(img, gamma=random.uniform(0.8, 1.2))\n",
    "            if random.random() < 0.5:\n",
    "                img = TF.adjust_contrast(img, contrast_factor=random.uniform(0.8, 1.25))\n",
    "\n",
    "        x = TF.to_tensor(img).repeat(3, 1, 1)\n",
    "        x = TF.normalize(x, IMAGENET_MEAN, IMAGENET_STD)\n",
    "\n",
    "        # robust binarization (0/255 masks)\n",
    "        m = torch.from_numpy((np.array(mask, dtype=np.uint8) >= 128).astype(np.int64))\n",
    "        return x, m\n",
    "\n",
    "# ------------------------------- Model: DeepLabV3-ResNet50 -----------------------------------\n",
    "def build_deeplab50(num_classes=1, freeze_backbone=True):\n",
    "    m = deeplabv3_resnet50(weights=DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1)\n",
    "    # Replace final classifier conv to 1 logit (foreground)\n",
    "    m.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "    if freeze_backbone:\n",
    "        for p in m.backbone.parameters(): p.requires_grad = False\n",
    "    return m.to(DEVICE)\n",
    "\n",
    "# ------------------------------- Losses ------------------------------------------------------\n",
    "def dice_loss_from_logits_binary(logits, target):\n",
    "    # logits: [N,1,H,W], target: [N,H,W] in {0,1}\n",
    "    probs = torch.sigmoid(logits).squeeze(1)  # [N,H,W]\n",
    "    target = target.float()\n",
    "    smooth = 1.0\n",
    "    inter  = (probs * target).sum(dim=(1,2))\n",
    "    union  = probs.sum(dim=(1,2)) + target.sum(dim=(1,2))\n",
    "    dice   = (2*inter + smooth) / (union + smooth)\n",
    "    return 1 - dice.mean()\n",
    "\n",
    "# ------------------------------- Post-processing --------------------------------------------\n",
    "def postprocess_keep_2_lungs(pred_prob, thresh=0.5):\n",
    "    \"\"\"\n",
    "    pred_prob: float tensor [H,W] in [0,1] -> returns uint8 mask {0,1}\n",
    "    Keeps two largest connected components and fills holes.\n",
    "    \"\"\"\n",
    "    binm = (pred_prob >= thresh).to(torch.uint8).cpu().numpy()\n",
    "    try:\n",
    "        from scipy import ndimage as ndi\n",
    "        lab, n = ndi.label(binm)\n",
    "        if n > 2:\n",
    "            sizes = ndi.sum(binm, lab, index=range(1, n+1))\n",
    "            keep = np.argsort(sizes)[-2:] + 1\n",
    "            binm = np.isin(lab, keep).astype(np.uint8)\n",
    "        binm = ndi.binary_fill_holes(binm, structure=np.ones((3,3))).astype(np.uint8)\n",
    "    except Exception:\n",
    "        # fallback hole fill via maxpool \"closing\"\n",
    "        t = torch.from_numpy(binm)[None,None].float()\n",
    "        t = F.max_pool2d(t, kernel_size=5, stride=1, padding=2)\n",
    "        binm = (t[0,0] > 0.5).to(torch.uint8).numpy()\n",
    "    return binm\n",
    "\n",
    "# ------------------------------- Metrics -----------------------------------------------------\n",
    "def mean_iou_binary(model, loader):\n",
    "    model.eval(); ious=[]\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE); yb = yb.cpu().numpy()\n",
    "            out  = model(xb)['out']              # [N,1,H,W]\n",
    "            prob = torch.sigmoid(out).cpu()      # [N,1,H,W]\n",
    "            for i in range(prob.size(0)):\n",
    "                pm = postprocess_keep_2_lungs(prob[i,0])\n",
    "                tm = yb[i].astype(np.uint8)\n",
    "                inter = (pm & tm).sum()\n",
    "                union = (pm | tm).sum()\n",
    "                if union > 0: ious.append(inter/union)\n",
    "    return float(np.mean(ious)) if ious else 0.0\n",
    "\n",
    "# ------------------------------- Training ----------------------------------------------------\n",
    "def train_seg_binary(model, train_loader, val_loader=None, epochs=40,\n",
    "                     lr_head=3e-4, lr_backbone=1e-4, unfreeze_at=12):\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    # start with head-only params (backbone frozen)\n",
    "    head_params = [p for n,p in model.named_parameters() if p.requires_grad and not n.startswith(\"backbone\")]\n",
    "    opt = torch.optim.AdamW([{\"params\": head_params, \"lr\": lr_head}])\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        # unfreeze backbone after warmup\n",
    "        if ep == unfreeze_at:\n",
    "            for p in model.backbone.parameters(): p.requires_grad = True\n",
    "            opt.add_param_group({\"params\": model.backbone.parameters(), \"lr\": lr_backbone})\n",
    "\n",
    "        model.train(); run_loss=0.0; nimg=0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE); yb = yb.to(DEVICE).float()  # [N,H,W]\n",
    "            out = model(xb)['out'].squeeze(1)               # [N,H,W]\n",
    "            loss = 0.5*bce(out, yb) + 0.5*dice_loss_from_logits_binary(out.unsqueeze(1), yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "            run_loss += loss.item() * xb.size(0); nimg += xb.size(0)\n",
    "\n",
    "        msg = f\"[DeepLab] E{ep:02d} train {run_loss/max(1,nimg):.4f}\"\n",
    "        if val_loader is not None:\n",
    "            miou = mean_iou_binary(model, val_loader)\n",
    "            msg += f\" | mIoU {miou:.3f}\"\n",
    "        print(msg)\n",
    "\n",
    "# ------------------------------- Use it ------------------------------------------------------\n",
    "# Requires: pairs_tr_1, pairs_te_1 already built from Segmentation01 lists.\n",
    "if 'pairs_tr_1' not in globals() or 'pairs_te_1' not in globals():\n",
    "    raise RuntimeError(\"pairs_tr_1 / pairs_te_1 are not defined. Build them from Segmentation01 first.\")\n",
    "\n",
    "# Loaders (bigger size helps DeepLab receptive field)\n",
    "ds_tr_4 = LungSegDataset(pairs_tr_1, size=512, augment=True)\n",
    "ds_te_4 = LungSegDataset(pairs_te_1, size=512, augment=False)\n",
    "ld_tr_4 = DataLoader(ds_tr_4, batch_size=6, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "ld_te_4 = DataLoader(ds_te_4, batch_size=6, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Build, train, evaluate\n",
    "model_4 = build_deeplab50(num_classes=1, freeze_backbone=True)\n",
    "train_seg_binary(model_4, ld_tr_4, ld_te_4, epochs=40, lr_head=3e-4, lr_backbone=1e-4, unfreeze_at=12)\n",
    "print(f\"[Task 4] Test mIoU (post-processed): {mean_iou_binary(model_4, ld_te_4):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a7971",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seg1_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 77\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# save examples from TEST loader\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m seg1_vis_dir \u001b[38;5;241m=\u001b[39m \u001b[43mseg1_dir\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m export_seg1_triptychs(model_4, ld_te_4, seg1_vis_dir, max_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, apply_post\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, thresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seg1_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# ---- Task 4: export \"image | GT | prediction\" triptychs (works with DeepLab/FCN) ----\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _denorm_to_u8(x3chw):\n",
    "    x = x3chw.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    x = (x * np.array(IMAGENET_STD) + np.array(IMAGENET_MEAN)).clip(0, 1)\n",
    "    return (x[..., 0] * 255).astype(np.uint8)   # grayscale repeated to 3ch\n",
    "\n",
    "def _binmask_to_u8(m):\n",
    "    return (m.astype(np.uint8) * 255)\n",
    "\n",
    "def _triptych_u8(img_u8, gt_u8, pr_u8, pad=6):\n",
    "    h, w = img_u8.shape\n",
    "    canvas = np.zeros((h, w * 3 + 2 * pad), dtype=np.uint8)\n",
    "    canvas[:, 0:w] = img_u8\n",
    "    canvas[:, w + pad:w * 2 + pad] = gt_u8\n",
    "    canvas[:, w * 2 + 2 * pad:] = pr_u8\n",
    "    return Image.fromarray(canvas)\n",
    "\n",
    "# optional: keep two largest components + fill holes (helps lungs look clean)\n",
    "def _postprocess_keep_two(bin_mask):\n",
    "    try:\n",
    "        from scipy import ndimage as ndi\n",
    "        lab, n = ndi.label(bin_mask)\n",
    "        if n > 2:\n",
    "            sizes = ndi.sum(bin_mask, lab, index=range(1, n+1))\n",
    "            keep = np.argsort(sizes)[-2:] + 1\n",
    "            bin_mask = np.isin(lab, keep).astype(np.uint8)\n",
    "        bin_mask = ndi.binary_fill_holes(bin_mask, structure=np.ones((3,3))).astype(np.uint8)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return bin_mask\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_seg1_triptychs(model, loader, out_dir: Path, max_n=12, apply_post=True, thresh=0.5):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    n = 0\n",
    "    model.eval()\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        out = model(xb)\n",
    "        # handle dict (torchvision) vs tensor (custom)\n",
    "        if isinstance(out, (dict, OrderedDict)):\n",
    "            logits = out['out']\n",
    "        else:\n",
    "            logits = out\n",
    "\n",
    "        if logits.shape[1] == 1:\n",
    "            # binary head -> sigmoid\n",
    "            prob = torch.sigmoid(logits).squeeze(1).cpu().numpy()  # [N,H,W]\n",
    "            pred = (prob >= thresh).astype(np.uint8)\n",
    "        else:\n",
    "            # multi-class head -> argmax\n",
    "            pred = logits.argmax(1).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        for i in range(xb.size(0)):\n",
    "            img_u8 = _denorm_to_u8(xb[i].cpu())\n",
    "            gt_u8  = _binmask_to_u8((yb[i].cpu().numpy() > 0).astype(np.uint8))\n",
    "            pr_bin = pred[i]\n",
    "            # if multi-class, treat any non-zero as foreground for viz\n",
    "            if pr_bin.ndim == 2 and pr_bin.max() > 1:\n",
    "                pr_bin = (pr_bin > 0).astype(np.uint8)\n",
    "            if apply_post:\n",
    "                pr_bin = _postprocess_keep_two(pr_bin)\n",
    "            pr_u8  = _binmask_to_u8(pr_bin)\n",
    "            panel  = _triptych_u8(img_u8, gt_u8, pr_u8, pad=8)\n",
    "            panel.save(out_dir / f\"seg1_{n:03d}.png\")\n",
    "            n += 1\n",
    "            if n >= max_n:\n",
    "                return\n",
    "\n",
    "# save examples from TEST loader\n",
    "seg1_vis_dir = seg1_dir / \"result\"\n",
    "export_seg1_triptychs(model_4, ld_te_4, seg1_vis_dir, max_n=12, apply_post=True, thresh=0.5)\n",
    "print(\"Saved triptychs to:\", seg1_vis_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa62d20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Device] CUDA OK -> using GPU\n",
      "[Segmentation02] ready at /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02\n",
      "[Seg2] Folders:\n",
      "  org_tr: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02/segmentation02/segmentation/org_train \n",
      "  lab_tr: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02/segmentation02/segmentation/label_train \n",
      "  org_te: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02/segmentation02/segmentation/org_test \n",
      "  lab_te: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02/segmentation02/segmentation/label_test\n",
      "[Seg2] 199 pairs built | 0 unmatched from org_train\n",
      "       example: segmentation02/segmentation/org_train/case123.bmp <-> segmentation02/segmentation/label_train/case123_label.png\n",
      "[Seg2] 48 pairs built | 0 unmatched from org_test\n",
      "       example: segmentation02/segmentation/org_test/case230.bmp <-> segmentation02/segmentation/label_test/case230_label.png\n",
      "[Seg2] final -> train=199 | test=48\n",
      "[Seg2][LUT] uniq codes: [0, 85, 170]\n",
      "[Seg2][LUT] inferred bg_code: 170\n",
      "[Seg2][LUT] mapping (orig -> new): {170: 0, 0: 1, 85: 2}\n",
      "[Seg2][LUT] n_classes (incl. background): 3\n",
      "[Seg2] Role inference: bg=0, lung_id=2, heart_id=1, counts=[8108043, 1089236, 2017394]\n",
      "[Seg2] Role inference: bg=0, lung_id=2, heart_id=1, counts=[8108043, 1089236, 2017394]\n",
      "[Seg2] class weights (remapped): [0.013347508385777473, 1.92365562915802, 1.0629968643188477]\n",
      "[sanity] n_classes=3, yb.min=0, yb.max=255\n",
      "[sanity] unique labels (sample): [0, 1, 2, 255]\n",
      "[sanity] logits shape: (4, 3, 512, 512)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E01   9.5s train 0.1932 | mIoU (no-bg) 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E02   9.5s train 0.0761 | mIoU (no-bg) 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E03   9.8s train 0.0539 | mIoU (no-bg) 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E04   9.7s train 0.0494 | mIoU (no-bg) 0.900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E05   9.8s train 0.0432 | mIoU (no-bg) 0.917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E06   9.5s train 0.0388 | mIoU (no-bg) 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E07   9.5s train 0.0381 | mIoU (no-bg) 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E08   9.5s train 0.0336 | mIoU (no-bg) 0.914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E09   9.6s train 0.0331 | mIoU (no-bg) 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E10   9.6s train 0.0306 | mIoU (no-bg) 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E11   9.5s train 0.0291 | mIoU (no-bg) 0.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E12   9.7s train 0.0278 | mIoU (no-bg) 0.922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E13   9.5s train 0.0274 | mIoU (no-bg) 0.923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E14   9.4s train 0.0260 | mIoU (no-bg) 0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E15   9.6s train 0.0251 | mIoU (no-bg) 0.925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E16   9.5s train 0.0250 | mIoU (no-bg) 0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E17   9.5s train 0.0263 | mIoU (no-bg) 0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E18   9.7s train 0.0239 | mIoU (no-bg) 0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E19   9.5s train 0.0223 | mIoU (no-bg) 0.933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E20   9.5s train 0.0215 | mIoU (no-bg) 0.924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E21   9.6s train 0.0210 | mIoU (no-bg) 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E22   9.4s train 0.0206 | mIoU (no-bg) 0.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E23   9.4s train 0.0213 | mIoU (no-bg) 0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E24   9.5s train 0.0207 | mIoU (no-bg) 0.929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E25   9.5s train 0.0191 | mIoU (no-bg) 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E26   9.5s train 0.0193 | mIoU (no-bg) 0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E27   9.5s train 0.0186 | mIoU (no-bg) 0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E28   9.5s train 0.0183 | mIoU (no-bg) 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E29   9.5s train 0.0174 | mIoU (no-bg) 0.930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E30   9.5s train 0.0176 | mIoU (no-bg) 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E31   9.4s train 0.0173 | mIoU (no-bg) 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E32   9.3s train 0.0165 | mIoU (no-bg) 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E33   9.4s train 0.0159 | mIoU (no-bg) 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E34  10.2s train 0.0171 | mIoU (no-bg) 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E35   9.4s train 0.0159 | mIoU (no-bg) 0.933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E36   9.6s train 0.0158 | mIoU (no-bg) 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E37   9.5s train 0.0157 | mIoU (no-bg) 0.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E38   9.5s train 0.0157 | mIoU (no-bg) 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E39   9.4s train 0.0153 | mIoU (no-bg) 0.932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[T5] E40   9.8s train 0.0161 | mIoU (no-bg) 0.931\n",
      "[Task 5] Per-class IoU: {'c0': 0.976567093950842, 'c1': 0.8962181881120231, 'c2': 0.9566745387567525}\n",
      "[Task 5] Mean IoU (no background): 0.926\n",
      "Saved Task 5 panels to: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02/result\n"
     ]
    }
   ],
   "source": [
    "# ===== Task 5: Organ multi-class segmentation (Segmentation02)  FINAL safe drop-in =====\n",
    "import os, gc, re, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from collections import Counter\n",
    "\n",
    "# used by post-processing / splitting and grayscale export\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "# ---------- Device (auto CPU fallback if CUDA is in a bad state) ----------\n",
    "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            _ = torch.ones(1, device=\"cuda\").sum().item()\n",
    "            torch.cuda.synchronize()\n",
    "            print(\"[Device] CUDA OK -> using GPU\")\n",
    "            return torch.device(\"cuda\")\n",
    "        except Exception as e:\n",
    "            print(f\"[Device] CUDA not usable due to prior device assert: {e}\\n[Device] Falling back to CPU.\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "DEVICE = get_device()\n",
    "# ---- HARD-DISABLE CUDA FOR THIS RUN IF WE FELL BACK TO CPU ----\n",
    "if DEVICE.type == \"cpu\" and torch.cuda.is_available():\n",
    "    print(\"[Device] Disabling CUDA APIs for this run (poisoned context).\")\n",
    "    # Make PyTorch think CUDA isn't available so optimizers don't touch CUDA at all\n",
    "    #torch.cuda.is_available = lambda: False  # type: ignore[misc]\n",
    "    # Belt & suspenders: stub this too in case any lib calls it\n",
    "    try:\n",
    "        torch.cuda.is_current_stream_capturing = lambda: False  # type: ignore[attr-defined]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD  = [0.229, 0.224, 0.225]\n",
    "IGNORE = 255\n",
    "\n",
    "# ---------- Data root ----------\n",
    "DATASETS.setdefault(\"Segmentation02\", \"http://imgcom.jsrt.or.jp/imgcom/wp-content/uploads/2018/11/Segmentation02.zip\")\n",
    "seg2_dir = download_if_needed(\"Segmentation02\", DATASETS[\"Segmentation02\"])\n",
    "\n",
    "# ---------- Folder discovery ----------\n",
    "IMG_EXTS  = (\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\")\n",
    "MASK_EXTS = (\".png\", \".bmp\", \".tif\", \".tiff\")\n",
    "\n",
    "def _score_path(p: Path, tokens):\n",
    "    s = p.as_posix().lower()\n",
    "    return sum(t in s for t in tokens)\n",
    "\n",
    "def _find_split_dir(root: Path, split: str, kind: str):\n",
    "    split_tokens = {\"train\": [\"train\"], \"test\": [\"test\"]}[split]\n",
    "    if kind == \"org\":\n",
    "        kind_tokens = [\"org\", \"image\", \"images\", \"img\"]; file_exts = IMG_EXTS\n",
    "    else:\n",
    "        kind_tokens = [\"label\", \"labels\", \"mask\", \"masks\"]; file_exts = MASK_EXTS\n",
    "    cands = [d for d in root.rglob(\"*\") if d.is_dir()]\n",
    "    filtered = []\n",
    "    for d in cands:\n",
    "        if any(q.is_file() and q.suffix.lower() in file_exts for q in d.rglob(\"*\")):\n",
    "            filtered.append(d)\n",
    "    filtered.sort(key=lambda d: (_score_path(d, split_tokens)+2*_score_path(d, kind_tokens), -len(d.as_posix())),\n",
    "                  reverse=True)\n",
    "    return filtered[0] if filtered else None\n",
    "\n",
    "org_tr = _find_split_dir(seg2_dir, \"train\", \"org\")\n",
    "lab_tr = _find_split_dir(seg2_dir, \"train\", \"label\")\n",
    "org_te = _find_split_dir(seg2_dir, \"test\",  \"org\")\n",
    "lab_te = _find_split_dir(seg2_dir, \"test\",  \"label\")\n",
    "\n",
    "if not (org_tr and lab_tr and org_te and lab_te):\n",
    "    raise RuntimeError(f\"Could not locate Segmentation02 subfolders.\\nroot={seg2_dir}\")\n",
    "\n",
    "print(\"[Seg2] Folders:\\n  org_tr:\", org_tr, \"\\n  lab_tr:\", lab_tr, \"\\n  org_te:\", org_te, \"\\n  lab_te:\", lab_te)\n",
    "\n",
    "# ---------- Pairing ----------\n",
    "def _stem_key(p: Path):\n",
    "    stem = p.stem.lower()\n",
    "    digits = \"\".join(ch for ch in stem if ch.isdigit())\n",
    "    return digits if digits else stem\n",
    "\n",
    "def pair_seg2(img_dir: Path, mask_dir: Path):\n",
    "    imgs  = [p for p in img_dir.rglob(\"*\") if p.is_file() and p.suffix.lower() in IMG_EXTS]\n",
    "    masks = [p for p in mask_dir.rglob(\"*\") if p.is_file() and p.suffix.lower() in MASK_EXTS]\n",
    "    if not imgs:  raise RuntimeError(f\"No images found under {img_dir}\")\n",
    "    if not masks: raise RuntimeError(f\"No masks found under {mask_dir}\")\n",
    "    m_by_key = {}\n",
    "    for q in masks:\n",
    "        m_by_key.setdefault(_stem_key(q), []).append(q)\n",
    "    pairs, misses = [], []\n",
    "    for ip in imgs:\n",
    "        cand = m_by_key.get(_stem_key(ip))\n",
    "        if cand:\n",
    "            cand.sort(key=lambda q: abs(len(q.stem)-len(ip.stem)))\n",
    "            pairs.append((ip, cand[0]))\n",
    "        else:\n",
    "            misses.append(ip.name)\n",
    "    print(f\"[Seg2] {len(pairs)} pairs built | {len(misses)} unmatched from {img_dir.name}\")\n",
    "    if pairs:\n",
    "        print(\"       example:\", pairs[0][0].relative_to(seg2_dir), \"<->\", pairs[0][1].relative_to(seg2_dir))\n",
    "    if not pairs: raise RuntimeError(f\"No imgmask pairs from {img_dir} and {mask_dir}\")\n",
    "    return pairs\n",
    "\n",
    "pairs_tr_2 = pair_seg2(org_tr, lab_tr)\n",
    "pairs_te_2 = pair_seg2(org_te, lab_te)\n",
    "print(f\"[Seg2] final -> train={len(pairs_tr_2)} | test={len(pairs_te_2)}\")\n",
    "\n",
    "\n",
    "# --- Model: DeepLabV3 ResNet50 with aux head and logits-only wrapper ---\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "def build_deeplabv3(num_classes, freeze_backbone=False, pretrained=True):\n",
    "    m = deeplabv3_resnet101(weights=DeepLabV3_ResNet101_Weights.DEFAULT, aux_loss=True)\n",
    "\n",
    "\n",
    "    # Replace heads to match our num_classes\n",
    "    m.classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "    if m.aux_classifier is not None:\n",
    "        m.aux_classifier[-1] = nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "\n",
    "    if freeze_backbone:\n",
    "        for p in m.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    return m  # NOTE: returns model with dict output: {'out': logits, 'aux': aux_logits}\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k, v in model.state_dict().items() if v.dtype.is_floating_point}\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for k, v in model.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1-self.decay)\n",
    "    @torch.no_grad()\n",
    "    def apply_to(self, model):\n",
    "        sd = model.state_dict()\n",
    "        sd.update({k: v for k, v in self.shadow.items()})\n",
    "        model.load_state_dict(sd)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Label LUT (contiguous 0..K-1; keep IGNORE=255) ----------\n",
    "def build_label_lut_auto_bg(pairs_a, pairs_b=None, ignore_index=255, sample=800):\n",
    "    \"\"\"\n",
    "    Scan masks, detect which raw code is 'background' as the most frequent value,\n",
    "    then map: bg_code -> 0 (background), every other seen code -> 1..K-1.\n",
    "    Unseen codes -> ignore_index.\n",
    "    \"\"\"\n",
    "    counts = Counter()\n",
    "    uniq = set()\n",
    "    all_pairs = list(pairs_a) + (list(pairs_b) if pairs_b else [])\n",
    "    for _, mp in all_pairs[:sample]:\n",
    "        arr = np.array(Image.open(mp))\n",
    "        uniq.update(np.unique(arr).tolist())\n",
    "        # count frequencies (skip ignore)\n",
    "        if ignore_index in uniq:\n",
    "            pass\n",
    "        vals, freqs = np.unique(arr[arr != ignore_index], return_counts=True)\n",
    "        counts.update(dict(zip(vals.tolist(), freqs.tolist())))\n",
    "\n",
    "    # Remove ignore from uniq set if present\n",
    "    if ignore_index in uniq:\n",
    "        uniq.discard(ignore_index)\n",
    "\n",
    "    if not counts:\n",
    "        raise RuntimeError(\"Could not compute frequencies; check your masks.\")\n",
    "\n",
    "    # Heuristic: background is the most frequent code\n",
    "    bg_code = max(counts.items(), key=lambda kv: kv[1])[0]\n",
    "\n",
    "    # Foreground are all other seen codes\n",
    "    fg_codes = sorted([v for v in uniq if v != bg_code])\n",
    "\n",
    "    max_code = max([ignore_index] + (list(uniq) if uniq else [0]))\n",
    "    lut = np.full(max_code + 1, ignore_index, dtype=np.int64)\n",
    "    lut[bg_code] = 0\n",
    "    for new_i, orig in enumerate(fg_codes, start=1):\n",
    "        lut[orig] = new_i\n",
    "\n",
    "    n_classes = 1 + len(fg_codes)\n",
    "    # Pretty print\n",
    "    mapping_preview = {int(bg_code): 0}\n",
    "    for orig in fg_codes:\n",
    "        mapping_preview[int(orig)] = int(lut[orig])\n",
    "    print(\"[Seg2][LUT] uniq codes:\", sorted(map(int, uniq)))\n",
    "    print(\"[Seg2][LUT] inferred bg_code:\", int(bg_code))\n",
    "    print(\"[Seg2][LUT] mapping (orig -> new):\", mapping_preview)\n",
    "    print(\"[Seg2][LUT] n_classes (incl. background):\", n_classes)\n",
    "\n",
    "    # Sanity: background should be majority after remap\n",
    "    if counts[bg_code] < 0.4 * sum(counts.values()):\n",
    "        print(\"[Seg2][WARN] Background not dominantdouble-check the masks/labels.\")\n",
    "    return lut, n_classes\n",
    "\n",
    "lut_np, n_classes = build_label_lut_auto_bg(pairs_tr_2, pairs_te_2, ignore_index=IGNORE)\n",
    "# after: lut_np, n_classes = build_label_lut_auto_bg(...)\n",
    "LUNG_ID, HEART_ID = infer_roles(pairs_tr_2 + pairs_te_2, lut_np, n_classes, ignore_index=IGNORE)\n",
    "\n",
    "\n",
    "def infer_roles(pairs, lut_np, n_classes, ignore_index=IGNORE, sample=400):\n",
    "    \"\"\"Decide which remapped id is lung vs heart by area (lung >> heart).\"\"\"\n",
    "    counts = np.zeros(n_classes, dtype=np.int64)\n",
    "    for _, mp in pairs[:sample]:\n",
    "        arr = np.array(Image.open(mp), dtype=np.int64)\n",
    "        arr = lut_np[arr]\n",
    "        arr = arr[arr != ignore_index]\n",
    "        if arr.size == 0: \n",
    "            continue\n",
    "        counts += np.bincount(arr, minlength=n_classes)\n",
    "    # foreground only\n",
    "    fg = counts[1:]\n",
    "    if len(fg) != 2:\n",
    "        raise RuntimeError(f\"Expected 2 foreground classes, got {len(fg)} with counts {counts.tolist()}\")\n",
    "    lung_id  = 1 + int(np.argmax(fg))   # larger area\n",
    "    heart_id = 1 + int(np.argmin(fg))   # smaller area\n",
    "    print(f\"[Seg2] Role inference: bg=0, lung_id={lung_id}, heart_id={heart_id}, counts={counts.tolist()}\")\n",
    "    return lung_id, heart_id\n",
    "\n",
    "# Use both train and test to be robust\n",
    "LUNG_ID, HEART_ID = infer_roles(pairs_tr_2 + pairs_te_2, lut_np, n_classes, ignore_index=IGNORE)\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "def _sample_weights(ds, k=5):\n",
    "    # upweight masks with SMALL heart and LARGE boundary length\n",
    "    ws = []\n",
    "    for _, mp in ds.pairs:\n",
    "        m = np.array(Image.open(mp))\n",
    "        heart = (m == 0).astype(np.uint8)\n",
    "        lungs = (m == 85).astype(np.uint8)\n",
    "        # proxy for boundary: gradient magnitude of lungs + heart\n",
    "        edge = cv2.Canny((lungs*255 + heart*255).astype(np.uint8), 50, 150)\n",
    "        w = 1.0 + 2.0*(heart.sum() < 20000) + 1.0*(edge.sum()/(255) > 2000)\n",
    "        ws.append(w)\n",
    "    ws = np.asarray(ws, np.float32); ws /= ws.mean()\n",
    "    return torch.as_tensor(ws, dtype=torch.double)\n",
    "\n",
    "\n",
    "sampler_tr = WeightedRandomSampler(_sample_weights(ds_tr_5), num_samples=len(ds_tr_5), replacement=True)\n",
    "ld_tr_5 = DataLoader(ds_tr_5, batch_size=4, sampler=sampler_tr, drop_last=True, num_workers=2 if pin else 0, pin_memory=pin)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Dataset & loaders ----------\n",
    "class OrganSegDataset(Dataset):\n",
    "    def __init__(self, pairs, lut_np, size=384, augment=True, ignore_index=IGNORE):\n",
    "        self.pairs = pairs\n",
    "        self.size = size\n",
    "        self.augment = augment\n",
    "        self.ignore_index = ignore_index\n",
    "        self.lut = lut_np\n",
    "    def __len__(self): return len(self.pairs)\n",
    "    def __getitem__(self, i):\n",
    "        ip, mp = self.pairs[i]\n",
    "        img  = Image.open(ip).convert(\"L\")\n",
    "        mask = Image.open(mp)\n",
    "        def _letterbox(im_pil, sz, mode_img=InterpolationMode.BILINEAR, mode_mask=InterpolationMode.NEAREST):\n",
    "            w, h = im_pil.size\n",
    "            scale = min(sz / h, sz / w)\n",
    "            nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "            im_res = TF.resize(im_pil, (nh, nw), mode_img)\n",
    "            pad_t = (sz - nh) // 2; pad_b = sz - nh - pad_t\n",
    "            pad_l = (sz - nw) // 2; pad_r = sz - nw - pad_l\n",
    "            im_pad = TF.pad(im_res, [pad_l, pad_t, pad_r, pad_b], fill=0)\n",
    "            return im_pad\n",
    "\n",
    "        def _letterbox_mask(msk_pil, sz):\n",
    "            w, h = msk_pil.size\n",
    "            scale = min(sz / h, sz / w)\n",
    "            nh, nw = int(round(h * scale)), int(round(w * scale))\n",
    "            m_res = TF.resize(msk_pil, (nh, nw), InterpolationMode.NEAREST)\n",
    "            pad_t = (sz - nh) // 2; pad_b = sz - nh - pad_t\n",
    "            pad_l = (sz - nw) // 2; pad_r = sz - nw - pad_l\n",
    "            m_pad = TF.pad(m_res, [pad_l, pad_t, pad_r, pad_b], fill=IGNORE)\n",
    "            return m_pad\n",
    "\n",
    "        img  = _letterbox(img,  self.size)\n",
    "        mask = _letterbox_mask(mask, self.size)\n",
    "        if self.augment:\n",
    "            if np.random.rand() < 0.5:\n",
    "                img = TF.hflip(img); mask = TF.hflip(mask)\n",
    "            # mild affine (no shear to avoid anatomy distortion)\n",
    "            if np.random.rand() < 0.25:\n",
    "                ang = float(np.random.uniform(-4, 4))\n",
    "                tx  = float(np.random.uniform(-0.02, 0.02) * img.size[1])\n",
    "                img = TF.affine(img, angle=ang, translate=(tx,0), scale=1.0, shear=0,\n",
    "                                interpolation=InterpolationMode.BILINEAR)\n",
    "                mask= TF.affine(mask, angle=ang, translate=(tx,0), scale=1.0, shear=0,\n",
    "                                interpolation=InterpolationMode.NEAREST)\n",
    "            # intensity jitter (gamma + slight brightness)\n",
    "            if np.random.rand() < 0.35:\n",
    "                g = float(np.random.uniform(0.8, 1.2))\n",
    "                img = TF.adjust_gamma(img, gamma=g)\n",
    "            if np.random.rand() < 0.35:\n",
    "                b = float(np.random.uniform(0.9, 1.1))\n",
    "                img = TF.adjust_brightness(img, b)\n",
    "        # CLAHE (contrast) 50% of the time\n",
    "        if self.augment and np.random.rand() < 0.5:\n",
    "            a = np.array(img, dtype=np.uint8)\n",
    "            a = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)).apply(a)\n",
    "            img = Image.fromarray(a)\n",
    "\n",
    "        x = TF.to_tensor(img).repeat(3,1,1)\n",
    "\n",
    "        x = TF.normalize(x, IMAGENET_MEAN, IMAGENET_STD)\n",
    "        m_orig = np.array(mask, dtype=np.int64)\n",
    "        if m_orig.max() >= self.lut.shape[0]:\n",
    "            ext = np.full(int(m_orig.max())+1, self.ignore_index, dtype=np.int64)\n",
    "            ext[:self.lut.shape[0]] = self.lut\n",
    "            self.lut = ext\n",
    "        m = self.lut[m_orig]\n",
    "        return x, torch.from_numpy(m)\n",
    "\n",
    "pin = (DEVICE.type == \"cuda\")\n",
    "ds_tr_5 = OrganSegDataset(pairs_tr_2, lut_np, size=512, augment=True,  ignore_index=IGNORE)\n",
    "ds_te_5 = OrganSegDataset(pairs_te_2, lut_np, size=384, augment=False, ignore_index=IGNORE)\n",
    "ld_tr_5 = DataLoader(\n",
    "    ds_tr_5,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    drop_last=True,               # <- prevents 1-sample BN issues\n",
    "    num_workers=2 if pin else 0,\n",
    "    pin_memory=pin,\n",
    ")\n",
    "ld_te_5 = DataLoader(ds_te_5, batch_size=4, shuffle=False,\n",
    "                     num_workers=2 if pin else 0, pin_memory=pin)\n",
    "\n",
    "\n",
    "# ---------- Class weights ----------\n",
    "def compute_class_weights_lut(pairs, lut_np, n_classes, ignore_index=IGNORE, sample=300):\n",
    "    counts = np.zeros(n_classes, dtype=np.float64)\n",
    "    for _, mp in pairs[:sample]:\n",
    "        arr = np.array(Image.open(mp), dtype=np.int64)\n",
    "        if arr.max() >= lut_np.shape[0]:\n",
    "            ext = np.full(int(arr.max())+1, ignore_index, dtype=np.int64)\n",
    "            ext[:lut_np.shape[0]] = lut_np\n",
    "            lut_np = ext\n",
    "        arr = lut_np[arr]\n",
    "        arr = arr[arr != ignore_index]\n",
    "        if arr.size == 0: continue\n",
    "        counts += np.bincount(arr, minlength=n_classes).astype(np.float64)\n",
    "    counts = np.maximum(counts, 1.0)\n",
    "    inv = 1.0 / counts\n",
    "    inv[0] *= 0.05\n",
    "    return torch.tensor(inv / inv.mean(), dtype=torch.float32)\n",
    "\n",
    "class_weights = compute_class_weights_lut(pairs_tr_2, lut_np, n_classes, ignore_index=IGNORE)\n",
    "print(\"[Seg2] class weights (remapped):\", class_weights.tolist())\n",
    "\n",
    "# ---------- Fixed Dice (mask ignore BEFORE one-hot; clamp to [0, C-1]) ----------\n",
    "def soft_dice_mc(logits, target, eps=1e-6, ignore_index=IGNORE):\n",
    "    C = logits.shape[1]\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "    t = target\n",
    "    if ignore_index is not None:\n",
    "        t = torch.where(t == ignore_index, torch.zeros_like(t), t)\n",
    "    t = t.clamp(0, C-1).long()\n",
    "\n",
    "    target_1h = torch.zeros_like(probs)\n",
    "    target_1h.scatter_(1, t.unsqueeze(1), 1.0)\n",
    "\n",
    "    if ignore_index is not None:\n",
    "        valid = (target != ignore_index).float().unsqueeze(1)\n",
    "        probs     = probs * valid\n",
    "        target_1h = target_1h * valid\n",
    "\n",
    "    inter = (probs[:, 1:] * target_1h[:, 1:]).sum(dim=(0,2,3))\n",
    "    denom = (probs[:, 1:] + target_1h[:, 1:]).sum(dim=(0,2,3))\n",
    "    return 1.0 - ((2*inter + eps) / (denom + eps)).mean()\n",
    "\n",
    "# Optional: turn semantic mask into per-organ instances for visualization/export\n",
    "def semantic_to_instances(lbl2d: np.ndarray, class_ids=(1,2,3)):\n",
    "    try:\n",
    "        from scipy.ndimage import label as cc_label\n",
    "    except Exception:\n",
    "        cc_label = None\n",
    "\n",
    "    insts = []\n",
    "    for cid in class_ids:\n",
    "        m = (lbl2d == cid).astype(np.uint8)\n",
    "        if m.sum() == 0:\n",
    "            continue\n",
    "        if cc_label is None:\n",
    "            # fallback: single instance\n",
    "            insts.append((cid, m.astype(bool)))\n",
    "            continue\n",
    "        n, comp = cc_label(m)\n",
    "        for i in range(1, n+1):\n",
    "            insts.append((cid, (comp == i)))\n",
    "    return insts\n",
    "\n",
    "# put near your viz code\n",
    "GRAY = { \"bg\":0, \"left\":85, \"heart\":160, \"right\":255 }  # feel free to tweak\n",
    "\n",
    "def to_gray_mask(lbl2d, mapping):\n",
    "    out = np.zeros_like(lbl2d, dtype=np.uint8)\n",
    "    for k,v in mapping.items():  # k is class-id, v is grayscale\n",
    "        out[lbl2d==k] = v\n",
    "    return out\n",
    "\n",
    "\n",
    "def lovasz_softmax_flat(probs, labels, classes='present'):\n",
    "    # minimal Lovasz-Softmax (multiclass). Assumes probs [P, C], labels [P]\n",
    "    def lovasz_grad(gt_sorted):\n",
    "        gts = gt_sorted.sum()\n",
    "        inter = gts - gt_sorted.cumsum(0)\n",
    "        union = gts + (1 - gt_sorted).cumsum(0)\n",
    "        jacc = 1. - inter / union\n",
    "        if gt_sorted.numel() > 1:\n",
    "            jacc[1:] = jacc[1:] - jacc[:-1]\n",
    "        return jacc\n",
    "    C = probs.size(1)\n",
    "    losses = []\n",
    "    for c in range(C):\n",
    "        fg = (labels == c).float()\n",
    "        if classes == 'present' and fg.sum() == 0:\n",
    "            continue\n",
    "        pc = probs[:, c]\n",
    "        errors = (fg - pc).abs()\n",
    "        errors_sorted, perm = torch.sort(errors, descending=True)\n",
    "        fg_sorted = fg[perm]\n",
    "        losses.append(torch.dot(errors_sorted, lovasz_grad(fg_sorted)))\n",
    "    return torch.stack(losses).mean() if losses else probs.new_tensor(0.)\n",
    "\n",
    "def lovasz_softmax(logits, labels, ignore_index=255):\n",
    "    C = logits.size(1)\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    mask = labels != ignore_index\n",
    "    probs, labels = probs.permute(0,2,3,1)[mask], labels[mask]\n",
    "    if probs.numel() == 0: return logits.new_tensor(0.)\n",
    "    return lovasz_softmax_flat(probs, labels)\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_triptych_gray(model, loader, out_dir: Path, ignore_index=255):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for xb, yb in loader:\n",
    "        out = model(xb.to(DEVICE))\n",
    "        logits = out['out'] if isinstance(out, dict) else out\n",
    "        pred = logits.argmax(1).cpu().numpy()\n",
    "        imgs = xb.numpy()\n",
    "        gts  = yb.numpy()\n",
    "        for i in range(imgs.shape[0]):\n",
    "            x = imgs[i,0] * IMAGENET_STD[0] + IMAGENET_MEAN[0]\n",
    "            x = (x.clip(0,1)*255).astype(np.uint8)\n",
    "            gt = gts[i].copy(); gt[gt==ignore_index]=0\n",
    "            pr = pred[i]\n",
    "\n",
    "            # after you split lungs (section 3), pr will already be {bg=0,left=1,heart=2,right=3}\n",
    "            gt_gray = to_gray_mask(gt, {0:GRAY[\"bg\"], 1:GRAY[\"left\"], 2:GRAY[\"heart\"], 3:GRAY[\"right\"]})\n",
    "            pr_gray = to_gray_mask(pr, {0:GRAY[\"bg\"], 1:GRAY[\"left\"], 2:GRAY[\"heart\"], 3:GRAY[\"right\"]})\n",
    "\n",
    "            h,w = x.shape; pad=6\n",
    "            canvas = np.zeros((h, 3*w+2*pad), np.uint8)\n",
    "            canvas[:, :w] = x\n",
    "            canvas[:, w+pad:2*w+pad] = gt_gray\n",
    "            canvas[:, 2*w+2*pad:]    = pr_gray\n",
    "            Image.fromarray(canvas).save(out_dir/f\"triptych_{np.random.randint(1e9)}.png\")\n",
    "\n",
    "# --- Clean semantic mask (remove speckles/holes and keep largest lungs) ---\n",
    "def clean_semantic3(pred, lung_id, heart_id):\n",
    "    out = pred.copy()\n",
    "\n",
    "    def _morph(m, k_open=3, k_close=5):\n",
    "        k1 = np.ones((k_open,k_open), np.uint8)\n",
    "        k2 = np.ones((k_close,k_close), np.uint8)\n",
    "        m = cv2.morphologyEx(m, cv2.MORPH_OPEN, k1)\n",
    "        m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, k2)\n",
    "        m = ndimage.binary_fill_holes(m).astype(np.uint8)\n",
    "        return m\n",
    "\n",
    "    lung  = _morph((out==lung_id).astype(np.uint8), 3, 7)\n",
    "\n",
    "    n, comp = cv2.connectedComponents(lung)  # n includes background\n",
    "    if n <= 1:\n",
    "        lung_keep = np.zeros_like(lung)\n",
    "    elif n == 2:\n",
    "        lung_keep = (comp == 1).astype(np.uint8)\n",
    "    else:\n",
    "        areas = [(comp==i).sum() for i in range(1, n)]\n",
    "        order = np.argsort(areas)\n",
    "        k1 = order[-1] + 1\n",
    "        k2 = order[-2] + 1\n",
    "        lung_keep = ((comp == k1) | (comp == k2)).astype(np.uint8)\n",
    "\n",
    "    heart = _morph((out==heart_id).astype(np.uint8), 3, 5)\n",
    "\n",
    "    clean = np.zeros_like(out, dtype=np.uint8)\n",
    "    clean[lung_keep==1]  = lung_id\n",
    "    clean[heart==1]      = heart_id\n",
    "    return clean\n",
    "\n",
    "\n",
    "\n",
    "# --- Split lungs into left/right (for visualization) ---\n",
    "def split_lungs_left_right(lbl, lung_id=1, heart_id=2):\n",
    "    m = (lbl==lung_id).astype(np.uint8)\n",
    "    n, comp = cv2.connectedComponents(m)\n",
    "    if n >= 3:\n",
    "        areas = [(comp==i).sum() for i in range(1,n)]\n",
    "        keep  = np.argsort(areas)[-2:]\n",
    "        m1, m2 = (comp==(keep[0]+1)), (comp==(keep[1]+1))\n",
    "    else:\n",
    "        # watershed fallback if lungs are fused\n",
    "        dist = cv2.distanceTransform(m, cv2.DIST_L2, 5)\n",
    "        maxima = (dist > 0.5*dist.max()).astype(np.uint8)\n",
    "        n2, markers = cv2.connectedComponents(maxima)\n",
    "        markers = markers+1\n",
    "        markers[m==0] = 0\n",
    "        m_color = cv2.cvtColor((m*255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        ws = cv2.watershed(m_color, markers.astype(np.int32))\n",
    "        m1, m2 = (ws==2), (ws==3)\n",
    "\n",
    "    # order by x-centroid (left < right)\n",
    "    x1 = np.mean(np.where(m1)[1]) if m1.any() else 0\n",
    "    x2 = np.mean(np.where(m2)[1]) if m2.any() else 1e9\n",
    "    left, right = (m1,m2) if x1 < x2 else (m2,m1)\n",
    "\n",
    "    out = np.zeros_like(lbl, dtype=np.uint8)\n",
    "    out[left]  = 1              # left lung\n",
    "    out[lbl==heart_id] = 2      # heart\n",
    "    out[right] = 3              # right lung\n",
    "    return out\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_tta(model, x):\n",
    "    model.eval()\n",
    "    scales = [0.75, 1.0, 1.25]\n",
    "    logits_all = []\n",
    "    B, C, H, W = x.shape\n",
    "    for s in scales:\n",
    "        x_s = F.interpolate(x, scale_factor=s, mode=\"bilinear\", align_corners=False) if s != 1.0 else x\n",
    "        y  = model(x_s); y  = y['out'] if isinstance(y, dict) else y\n",
    "        y  = F.interpolate(y, size=(H,W), mode=\"bilinear\", align_corners=False)\n",
    "        logits_all.append(y)\n",
    "        y2 = model(torch.flip(x_s, dims=[-1])); y2 = y2['out'] if isinstance(y2, dict) else y2\n",
    "        y2 = torch.flip(y2, dims=[-1])\n",
    "        y2 = F.interpolate(y2, size=(H,W), mode=\"bilinear\", align_corners=False)\n",
    "        logits_all.append(y2)\n",
    "    return torch.softmax(torch.stack(logits_all,0).mean(0), dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Metrics ----------\n",
    "@torch.no_grad()\n",
    "@torch.no_grad()\n",
    "def eval_iou_mc(model, loader, n_classes, ignore_index=IGNORE, postprocess=True, use_tta=True):\n",
    "    model.eval()\n",
    "    inter = np.zeros(n_classes, dtype=np.float64)\n",
    "    union = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(DEVICE, non_blocking=True) if pin else xb.to(DEVICE)\n",
    "        probs = predict_tta(model, xb) if use_tta else (model(xb)['out'] if isinstance(model(xb), dict) else model(xb)).softmax(1)\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        gt   = yb.numpy()\n",
    "\n",
    "        for j in range(pred.shape[0]):\n",
    "            p = clean_semantic3(pred[j], LUNG_ID, HEART_ID) if postprocess else pred[j]\n",
    "            v = (gt[j] != ignore_index)\n",
    "            for c in range(n_classes):\n",
    "                P = (p == c) & v\n",
    "                T = (gt[j] == c) & v\n",
    "                inter[c] += (P & T).sum()\n",
    "                union[c] += (P | T).sum()\n",
    "\n",
    "    iou = inter / np.maximum(union, 1)\n",
    "    idx = [c for c in range(1, n_classes) if union[c] > 0]\n",
    "    miou = iou[idx].mean() if idx else 0.0\n",
    "    return iou, float(miou)\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Training ----------\n",
    "from time import time\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    tqdm = None\n",
    "\n",
    "def train_seg_mc(model, train_loader, val_loader, n_classes, epochs=30, lr=3e-4,\n",
    "                 ignore_index=IGNORE, class_weights=None, dice_fn=None):\n",
    "    if dice_fn is None:\n",
    "        dice_fn = soft_dice_mc\n",
    "\n",
    "    ce = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE) if class_weights is not None else None,\n",
    "                             ignore_index=ignore_index)\n",
    "    opt = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                            lr=lr, weight_decay=1e-4)\n",
    "    sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=epochs)\n",
    "    ema = EMA(model, decay=0.999)\n",
    "\n",
    "    best_miou, best = -1.0, None\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); tot = 0.0\n",
    "        iterator = train_loader\n",
    "        if tqdm is not None:\n",
    "            iterator = tqdm(train_loader, desc=f\"Epoch {ep}/{epochs}\", leave=False)\n",
    "\n",
    "        t0 = time()\n",
    "        for xb, yb in iterator:\n",
    "            xb, yb = xb.to(DEVICE, non_blocking=False), yb.to(DEVICE, non_blocking=False)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            out = model(xb)\n",
    "            logits = out['out'] if isinstance(out, dict) else out\n",
    "            aux_logits = out.get('aux', None) if isinstance(out, dict) else None\n",
    "\n",
    "            def focal_ce(logits, target, gamma=2.0, ignore_index=IGNORE, weight=None):\n",
    "    # standard focal on CE\n",
    "                logp = F.log_softmax(logits, dim=1)\n",
    "                ce   = F.nll_loss(logp, target, ignore_index=ignore_index, reduction='none', weight=weight)\n",
    "                with torch.no_grad():\n",
    "                    p = torch.exp(-ce)\n",
    "                return ((1 - p) ** gamma * ce).mean()\n",
    "\n",
    "            l_focal = focal_ce(logits, yb, gamma=2.0, ignore_index=ignore_index, weight=class_weights.to(DEVICE))\n",
    "            l_dice  = dice_fn(logits, yb, ignore_index=ignore_index)\n",
    "            l_lov   = lovasz_softmax(logits, yb, ignore_index=ignore_index)\n",
    "            loss = 0.35*l_focal + 0.30*l_dice + 0.35*l_lov\n",
    "\n",
    "            if aux_logits is not None:\n",
    "                loss = loss + 0.2 * ce(aux_logits, yb)  # aux drives backbone early\n",
    "\n",
    "            loss.backward(); opt.step()\n",
    "            ema.update(model)       \n",
    "            tot += loss.item() * xb.size(0)\n",
    "\n",
    "            # If tqdm not available, print every 5 batches for visibility\n",
    "            if tqdm is None and (tot // xb.size(0)) % 5 == 0:\n",
    "                print(f\"  [E{ep:02d}] running loss {loss.item():.3f}\", flush=True)\n",
    "\n",
    "        sched.step()\n",
    "        _, miou = eval_iou_mc(model, val_loader, n_classes, ignore_index=ignore_index)\n",
    "        dt = time() - t0\n",
    "        print(f\"[T5] E{ep:02d} {dt:5.1f}s train {tot/len(train_loader.dataset):.4f} | mIoU (no-bg) {miou:.3f}\", flush=True)\n",
    "\n",
    "        if miou > best_miou:\n",
    "            best_miou, best = miou, {k: v.detach().cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "    if best is not None:\n",
    "        ema.apply_to(model)\n",
    "        model.load_state_dict({k: v.to(DEVICE) for k, v in model.state_dict().items()})\n",
    "    return model\n",
    "\n",
    "\n",
    "# ---------- Model builder (fallback if your Task-4 builder absent) ----------\n",
    "if \"build_fcn_resnet18\" not in globals():\n",
    "    from torchvision.models import resnet18\n",
    "    try:\n",
    "        from torchvision.models import ResNet18_Weights\n",
    "        _weights = ResNet18_Weights.DEFAULT\n",
    "    except Exception:\n",
    "        _weights = None\n",
    "    class _FCN32s(nn.Module):\n",
    "        def __init__(self, num_classes=2, freeze_backbone=True):\n",
    "            super().__init__()\n",
    "            m = resnet18(weights=_weights)\n",
    "            self.backbone = nn.Sequential(m.conv1, m.bn1, m.relu, m.maxpool, m.layer1, m.layer2, m.layer3, m.layer4)\n",
    "            if freeze_backbone:\n",
    "                for p in self.backbone.parameters(): p.requires_grad = False\n",
    "            self.head = nn.Conv2d(512, num_classes, 1)\n",
    "        def forward(self, x):\n",
    "            H,W = x.shape[-2:]\n",
    "            f = self.backbone(x)\n",
    "            lo = self.head(f)\n",
    "            return F.interpolate(lo, size=(H,W), mode=\"bilinear\", align_corners=False)\n",
    "    def build_fcn_resnet18(num_classes=2, freeze_backbone=True):\n",
    "        return _FCN32s(num_classes, freeze_backbone)\n",
    "\n",
    "# ---------- Reset any stale model (no CUDA cache call here) ----------\n",
    "try: del model_5\n",
    "except NameError: pass\n",
    "gc.collect()\n",
    "\n",
    "def freeze_bn(m: nn.Module):\n",
    "    for mod in m.modules():\n",
    "        if isinstance(mod, nn.BatchNorm2d):\n",
    "            mod.eval()                 # use running stats\n",
    "            for p in mod.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "model_5 = build_deeplabv3(n_classes, freeze_backbone=False, pretrained=True).to(DEVICE)\n",
    "def freeze_backbone_early(m):\n",
    "    # keep stem+layer1 fixed, allow layer3/4 to adapt\n",
    "    for name, p in m.backbone.named_parameters():\n",
    "        if name.startswith((\"conv1\",\"bn1\",\"layer1\",\"layer2\")):\n",
    "            p.requires_grad = False\n",
    "\n",
    "def set_bn_eval_early(m):\n",
    "    for n, mod in m.backbone.named_modules():\n",
    "        if isinstance(mod, nn.BatchNorm2d) and (n.startswith(\"layer1\") or n.startswith(\"layer2\") or n.startswith(\"bn1\")):\n",
    "            mod.eval()\n",
    "            for p in mod.parameters(): p.requires_grad = False\n",
    "\n",
    "freeze_backbone_early(model_5)\n",
    "set_bn_eval_early(model_5)     # keep BN stable early; BN trainable in layer3/4 + heads\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------- Sanity checks ----------\n",
    "xb, yb = next(iter(ld_tr_5))\n",
    "print(f\"[sanity] n_classes={n_classes}, yb.min={int(yb.min())}, yb.max={int(yb.max())}\")\n",
    "print(\"[sanity] unique labels (sample):\", torch.unique(yb).tolist())\n",
    "assert ((yb == IGNORE) | ((yb >= 0) & (yb < n_classes))).all(), \"Targets contain out-of-range class ids.\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model_5(xb.to(DEVICE))\n",
    "    logits = out['out'] if isinstance(out, dict) else out\n",
    "print(\"[sanity] logits shape:\", tuple(logits.shape))\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = soft_dice_mc(logits, yb.to(DEVICE), ignore_index=IGNORE)\n",
    "\n",
    "\n",
    "# ---------- Train ----------\n",
    "model_5 = train_seg_mc(model_5, ld_tr_5, ld_te_5, n_classes,\n",
    "                       epochs=40, lr=3e-4, ignore_index=IGNORE,\n",
    "                       class_weights=class_weights, dice_fn=soft_dice_mc)\n",
    "\n",
    "from torch.optim.swa_utils import AveragedModel\n",
    "\n",
    "def swa_average(model, loader, epochs=5):\n",
    "    swa_model = AveragedModel(model).to(DEVICE)\n",
    "    for _ in range(epochs):\n",
    "        for xb, _ in loader:\n",
    "            with torch.no_grad():\n",
    "                _ = model(xb.to(DEVICE))  # forward to keep BN happy if not frozen\n",
    "        swa_model.update_parameters(model)\n",
    "    return swa_model\n",
    "\n",
    "# after training (and before final eval)\n",
    "swa_m = swa_average(model_5, ld_tr_5, epochs=5)\n",
    "model_5.load_state_dict(swa_m.module.state_dict(), strict=False)\n",
    "model_5.train()\n",
    "with torch.no_grad():\n",
    "    for xb, _ in ld_tr_5:\n",
    "        _ = model_5(xb.to(DEVICE))  # refresh BN running stats\n",
    "model_5.eval()\n",
    "\n",
    "\n",
    "# ---------- Evaluate ----------\n",
    "iou, miou = eval_iou_mc(model_5, ld_te_5, n_classes, ignore_index=IGNORE)\n",
    "print(\"[Task 5] Per-class IoU:\", {f\"c{c}\": float(i) for c,i in enumerate(iou)})\n",
    "print(f\"[Task 5] Mean IoU (no background): {miou:.3f}\")\n",
    "\n",
    "# ---------- Visualization ----------\n",
    "GRAY = {\"bg\":0, \"left\":85, \"heart\":160, \"right\":255}\n",
    "\n",
    "def to_gray_mask(lbl2d, mapping):\n",
    "    out = np.zeros_like(lbl2d, dtype=np.uint8)\n",
    "    for k,v in mapping.items():\n",
    "        out[lbl2d==k] = v\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def export_triptych_gray(\n",
    "    model,\n",
    "    loader,\n",
    "    out_dir: Path,\n",
    "    ignore_index=IGNORE,\n",
    "    use_tta=True,\n",
    "    max_n=12,\n",
    "    prefix=\"triptych\",\n",
    "    view=\"lr\"  # \"lr\" = split left/right for display, \"semantic\" = 3-class\n",
    "):\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    saved = 0\n",
    "\n",
    "    for bidx, (xb, yb) in enumerate(loader):\n",
    "        if saved >= max_n: break\n",
    "\n",
    "        xb_dev = xb.to(DEVICE, non_blocking=True) if pin else xb.to(DEVICE)\n",
    "        probs = predict_tta(model, xb_dev) if use_tta else \\\n",
    "                ((model(xb_dev)['out'] if isinstance(model(xb_dev), dict) else model(xb_dev)).softmax(1))\n",
    "\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        imgs = xb.detach().cpu().numpy()\n",
    "        gts  = yb.numpy()\n",
    "\n",
    "        for i in range(imgs.shape[0]):\n",
    "            if saved >= max_n: break\n",
    "\n",
    "            # de-normalize the image (channel 0 duplicated)\n",
    "            x = imgs[i,0] * IMAGENET_STD[0] + IMAGENET_MEAN[0]\n",
    "            x = (x.clip(0,1) * 255).astype(np.uint8)\n",
    "\n",
    "            # 3-class semantic prediction with the correct IDs\n",
    "            gt_sem = gts[i].copy()\n",
    "            gt_sem[gt_sem == ignore_index] = 0\n",
    "            pr_sem = clean_semantic3(pred[i], LUNG_ID, HEART_ID)\n",
    "\n",
    "            if view == \"semantic\":\n",
    "                # bg/heart/lung view\n",
    "                sem_map = {0:GRAY[\"bg\"], HEART_ID:GRAY[\"heart\"], LUNG_ID:GRAY[\"right\"]}\n",
    "# (any neutral lung shade works; the key is not reusing \"left\" which is for LR view)\n",
    "\n",
    "                gt_gray = to_gray_mask(gt_sem, sem_map)\n",
    "                pr_gray = to_gray_mask(pr_sem, sem_map)\n",
    "            else:\n",
    "                # prettier left/right split for display\n",
    "                gt_lr = split_lungs_left_right(gt_sem, lung_id=LUNG_ID, heart_id=HEART_ID)\n",
    "                pr_lr = split_lungs_left_right(pr_sem, lung_id=LUNG_ID, heart_id=HEART_ID)\n",
    "                lr_map = {0:GRAY[\"bg\"], 1:GRAY[\"left\"], 2:GRAY[\"heart\"], 3:GRAY[\"right\"]}\n",
    "                gt_gray = to_gray_mask(gt_lr, lr_map)\n",
    "                pr_gray = to_gray_mask(pr_lr, lr_map)\n",
    "\n",
    "            h,w = x.shape; pad = 6\n",
    "            canvas = np.zeros((h, 3*w + 2*pad), np.uint8)\n",
    "            canvas[:, :w] = x\n",
    "            canvas[:, w+pad:2*w+pad] = gt_gray\n",
    "            canvas[:, 2*w+2*pad:]    = pr_gray\n",
    "\n",
    "            global_idx = bidx * loader.batch_size + i\n",
    "            stem = Path(loader.dataset.pairs[global_idx][0]).stem\n",
    "            Image.fromarray(canvas).save(out_dir / f\"{prefix}_{global_idx:03d}_{stem}.png\")\n",
    "\n",
    "            saved += 1\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "seg2_vis_dir = seg2_dir / \"result\"\n",
    "export_triptych_gray(model_5, ld_te_5, seg2_vis_dir, max_n=8, view=\"lr\")\n",
    "\n",
    "print(\"Saved Task 5 panels to:\", seg2_vis_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6cc89941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Task 6: Localize organs (3 boxes, robust split + heart clamp/synthesis) =====\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch, cv2\n",
    "\n",
    "# ---------- small helpers ----------\n",
    "def _denorm_to_uint8(x_chw):\n",
    "    x = x_chw[0].detach().cpu().numpy()\n",
    "    x = x * IMAGENET_STD[0] + IMAGENET_MEAN[0]\n",
    "    x = np.clip(x, 0, 1)\n",
    "    return (x * 255.0 + 0.5).astype(np.uint8)\n",
    "\n",
    "def bbox_from_mask(mask_bool, min_pixels=50, pad=0, clip_hw=None):\n",
    "    if mask_bool is None:\n",
    "        return None\n",
    "    ys, xs = np.where(mask_bool)\n",
    "    if ys.size < min_pixels:\n",
    "        return None\n",
    "    y1, y2 = int(ys.min()), int(ys.max())\n",
    "    x1, x2 = int(xs.min()), int(xs.max())\n",
    "    if pad > 0:\n",
    "        y1 -= pad; x1 -= pad; y2 += pad; x2 += pad\n",
    "    if clip_hw is not None:\n",
    "        H, W = clip_hw\n",
    "        x1 = max(0, x1); y1 = max(0, y1)\n",
    "        x2 = min(W-1, x2); y2 = min(H-1, y2)\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "def _draw(bgr, box, color, label, t=3):\n",
    "    if box is None: return bgr\n",
    "    x1,y1,x2,y2 = box\n",
    "    cv2.rectangle(bgr, (x1,y1), (x2,y2), color, t)\n",
    "    cv2.putText(bgr, label, (x1, max(0, y1-6)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2, cv2.LINE_AA)\n",
    "    return bgr\n",
    "\n",
    "# ---------- robust lung split (always returns left & right) ----------\n",
    "def split_lungs_kmeans(lung_mask: np.ndarray):\n",
    "    \"\"\"Split total lung mask into left/right by clustering x-coordinates.\"\"\"\n",
    "    ys, xs = np.where(lung_mask)\n",
    "    if xs.size == 0:\n",
    "        return np.zeros_like(lung_mask, bool), np.zeros_like(lung_mask, bool)\n",
    "    # 1-D k-means on x\n",
    "    xs1 = xs.astype(np.float32).reshape(-1,1)\n",
    "    # kmeans may fail on degenerate inputs; guard with try\n",
    "    try:\n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 50, 0.5)\n",
    "        _compactness, labels, centers = cv2.kmeans(xs1, 2, None, criteria, 5, cv2.KMEANS_PP_CENTERS)\n",
    "        centers = centers.squeeze()\n",
    "        left_center_idx = int(np.argmin(centers))\n",
    "        right_center_idx = 1 - left_center_idx\n",
    "        left_sel  = (labels.ravel() == left_center_idx)\n",
    "        right_sel = (labels.ravel() == right_center_idx)\n",
    "    except Exception:\n",
    "        # fallback: median split on x\n",
    "        xmid = int(np.median(xs))\n",
    "        left_sel  = xs < xmid\n",
    "        right_sel = xs >= xmid\n",
    "\n",
    "    mL = np.zeros_like(lung_mask, bool); mR = np.zeros_like(lung_mask, bool)\n",
    "    mL[ys[left_sel], xs[left_sel]]   = True\n",
    "    mR[ys[right_sel], xs[right_sel]] = True\n",
    "\n",
    "    # quick cleanups\n",
    "    k3 = np.ones((3,3), np.uint8)\n",
    "    mL = cv2.morphologyEx(mL.astype(np.uint8), cv2.MORPH_CLOSE, k3).astype(bool)\n",
    "    mR = cv2.morphologyEx(mR.astype(np.uint8), cv2.MORPH_CLOSE, k3).astype(bool)\n",
    "    return mL, mR\n",
    "\n",
    "# ---------- heart box post-processing / synthesis ----------\n",
    "def clamp_and_fill_heart(heart_mask, mL, mR, H, W, min_pixels=500):\n",
    "    \"\"\"Clamp heart to lung vertical band; synthesize if weak/empty.\"\"\"\n",
    "    thorax_left  = np.where(mL.any(0))[0]\n",
    "    thorax_right = np.where(mR.any(0))[0]\n",
    "    if thorax_left.size == 0 or thorax_right.size == 0:\n",
    "        # Degenerate; return None\n",
    "        return None\n",
    "\n",
    "    # Lung vertical extent:\n",
    "    lung_band = (mL | mR)\n",
    "    ys = np.where(lung_band.any(1))[0]\n",
    "    y_top = int(ys.min()); y_bot = int(ys.max())\n",
    "    y_pad = int(0.02 * H)\n",
    "    y_top = max(0, y_top + y_pad)\n",
    "    y_bot = min(H-1, y_bot - y_pad)\n",
    "\n",
    "    # Use predicted heart if present; otherwise synthesize between lungs\n",
    "    use_pred = heart_mask.sum() >= min_pixels\n",
    "    if use_pred:\n",
    "        hm = heart_mask.copy()\n",
    "        # keep only middle third horizontally (avoid ribs/diaphragm noise)\n",
    "        xL = thorax_left.min(); xR = thorax_right.max()\n",
    "        x_mid = (xL + xR) // 2\n",
    "        x_halfband = int(0.18 * (xR - xL))\n",
    "        xmin = max(0, x_mid - x_halfband); xmax = min(W-1, x_mid + x_halfband)\n",
    "        band = np.zeros_like(hm); band[:, xmin:xmax] = True\n",
    "        hm &= band\n",
    "        box = bbox_from_mask(hm, min_pixels=min_pixels//2, pad=6, clip_hw=(H,W))\n",
    "        if box is not None:\n",
    "            x1,y1,x2,y2 = box\n",
    "            # clamp vertically to lung band\n",
    "            y1 = max(y_top, y1); y2 = min(y_bot, y2)\n",
    "            # enforce reasonable width: 0.200.45 of thorax width\n",
    "            tw  = (thorax_right.max() - thorax_left.min())\n",
    "            w   = x2 - x1 + 1\n",
    "            w = int(np.clip(w, 0.20*tw, 0.45*tw))\n",
    "            # recenter around x_mid\n",
    "            x1 = int(np.clip(x_mid - w//2, 0, W-1)); x2 = int(np.clip(x1 + w, 0, W-1))\n",
    "            return (x1, y1, x2, y2)\n",
    "\n",
    "    # Synthesize: between lungs, inside lung vertical band\n",
    "    xL = thorax_left.max()     # inner edge of left lung\n",
    "    xR = thorax_right.min()    # inner edge of right lung\n",
    "    x_mid = (xL + xR) // 2\n",
    "    tw = (thorax_right.max() - thorax_left.min())\n",
    "    w  = int(np.clip(0.30 * tw, 0.20*tw, 0.45*tw))\n",
    "    x1 = int(np.clip(x_mid - w//2, 0, W-1)); x2 = int(np.clip(x1 + w, 0, W-1))\n",
    "    # vertical: middle 70% of lung band\n",
    "    h  = int(0.70 * (y_bot - y_top + 1))\n",
    "    y1 = int(y_top + 0.15 * (y_bot - y_top + 1))\n",
    "    y2 = int(np.clip(y1 + h, 0, H-1))\n",
    "    return (x1, y1, x2, y2)\n",
    "\n",
    "@torch.no_grad()\n",
    "def localize_organs_from_loader(\n",
    "    model,\n",
    "    loader,\n",
    "    lung_id=LUNG_ID,\n",
    "    heart_id=HEART_ID,\n",
    "    out_dir=\"task6_boxes_vis\",\n",
    "    csv_path=\"task6_boxes.csv\",\n",
    "    use_tta=True,\n",
    "    min_pixels=300,      # ignore tiny specks\n",
    "    pad_pixels=6,\n",
    "    max_images=8,\n",
    "):\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dev = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    rows = []; saved = 0\n",
    "    COL_LEFT  = (0,255,255)   # yellow\n",
    "    COL_RIGHT = (255,255,0)   # cyan\n",
    "    COL_HEART = (203, 0, 255) # magenta\n",
    "\n",
    "    for bidx, (xb, _) in enumerate(loader):\n",
    "        if saved >= max_images: break\n",
    "        xb = xb.to(dev, non_blocking=True)\n",
    "        probs = predict_tta(model, xb) if use_tta else ((model(xb)['out'] if isinstance(model(xb), dict) else model(xb)).softmax(1))\n",
    "        pred  = probs.argmax(1).cpu().numpy()\n",
    "\n",
    "        for i in range(xb.size(0)):\n",
    "            if saved >= max_images: break\n",
    "            img_u8 = _denorm_to_uint8(xb[i]); H,W = img_u8.shape\n",
    "\n",
    "            sem = clean_semantic3(pred[i], lung_id, heart_id)\n",
    "            m_lung = (sem == lung_id)\n",
    "            # robust split\n",
    "            m_left, m_right = split_lungs_kmeans(m_lung)\n",
    "\n",
    "            # left coverage fix: ensure left covers all lung pixels left of the mid-split\n",
    "            # determine spine/midline as midpoint between inner edges of the two masks\n",
    "            xsL = np.where(m_left.any(0))[0];  xsR = np.where(m_right.any(0))[0]\n",
    "            if xsL.size and xsR.size:\n",
    "                mid = (xsL.max() + xsR.min()) // 2\n",
    "                m_left |= (m_lung & (np.arange(W)[None,:] <= mid))\n",
    "                m_right|= (m_lung & (np.arange(W)[None,:] >  mid))\n",
    "\n",
    "            # heart clamp/synthesis\n",
    "            m_heart_pred = (sem == heart_id)\n",
    "            box_heart = clamp_and_fill_heart(m_heart_pred, m_left, m_right, H, W, min_pixels=max(400, min_pixels))\n",
    "\n",
    "            # final boxes\n",
    "            box_left  = bbox_from_mask(m_left,  min_pixels=min_pixels, pad=pad_pixels, clip_hw=(H,W))\n",
    "            box_right = bbox_from_mask(m_right, min_pixels=min_pixels, pad=pad_pixels, clip_hw=(H,W))\n",
    "\n",
    "            # draw\n",
    "            vis = cv2.cvtColor(img_u8, cv2.COLOR_GRAY2BGR)\n",
    "            vis = _draw(vis, box_left,  COL_LEFT,  \"left lung\")\n",
    "            vis = _draw(vis, box_right, COL_RIGHT, \"right lung\")\n",
    "            vis = _draw(vis, box_heart, COL_HEART, \"heart\")\n",
    "\n",
    "            # naming\n",
    "            try:\n",
    "                global_idx = bidx*loader.batch_size + i\n",
    "                name = Path(loader.dataset.pairs[min(global_idx, len(loader.dataset.pairs)-1)][0]).stem\n",
    "            except Exception:\n",
    "                name = f\"img_{bidx:04d}_{i:02d}\"\n",
    "            cv2.imwrite(str(out_dir / f\"{name}_boxes.png\"), vis)\n",
    "\n",
    "            def _row(b): return (-1,-1,-1,-1) if b is None else b\n",
    "            lx1,ly1,lx2,ly2 = _row(box_left)\n",
    "            rx1,ry1,rx2,ry2 = _row(box_right)\n",
    "            hx1,hy1,hx2,hy2 = _row(box_heart)\n",
    "\n",
    "            rows.append({\n",
    "                \"image_id\": name,\n",
    "                \"left_x1\": lx1,  \"left_y1\": ly1,  \"left_x2\": lx2,  \"left_y2\": ly2,\n",
    "                \"right_x1\": rx1, \"right_y1\": ry1, \"right_x2\": rx2, \"right_y2\": ry2,\n",
    "                \"heart_x1\": hx1, \"heart_y1\": hy1, \"heart_x2\": hx2, \"heart_y2\": hy2,\n",
    "                \"height\": H, \"width\": W,\n",
    "            })\n",
    "            saved += 1\n",
    "\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(\n",
    "            f,\n",
    "            fieldnames=[\n",
    "                \"image_id\",\n",
    "                \"left_x1\",\"left_y1\",\"left_x2\",\"left_y2\",\n",
    "                \"right_x1\",\"right_y1\",\"right_x2\",\"right_y2\",\n",
    "                \"heart_x1\",\"heart_y1\",\"heart_x2\",\"heart_y2\",\n",
    "                \"height\",\"width\",\n",
    "            ]\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"[Task 6] Saved {len(rows)} images with boxes to: {out_dir}\")\n",
    "    print(f\"[Task 6] CSV written to: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5c04576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task 6] Saved 8 images with boxes to: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02/task6_vis\n",
      "[Task 6] CSV written to: /scratch/hdharmen/ASU/CSE 507/data/jsrt/Segmentation02/task6_boxes.csv\n"
     ]
    }
   ],
   "source": [
    "task6_out_dir = Path(seg2_dir) / \"task6_vis\"\n",
    "task6_csv     = Path(seg2_dir) / \"task6_boxes.csv\"\n",
    "\n",
    "localize_organs_from_loader(\n",
    "    model_5,\n",
    "    ld_te_5,                  # uses your test loader\n",
    "    lung_id=LUNG_ID,\n",
    "    heart_id=HEART_ID,\n",
    "    out_dir=str(task6_out_dir),\n",
    "    csv_path=str(task6_csv),\n",
    "    use_tta=True,\n",
    "    min_pixels=200,\n",
    "    pad_pixels=6,\n",
    "    max_images=8,             # << only save 8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303b3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cse507",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
